<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[JRald]]></title><description><![CDATA[Yet another blog about tech stuff]]></description><link>https://gquintana.github.io</link><generator>RSS for Node</generator><lastBuildDate>Fri, 07 Jun 2019 18:25:06 GMT</lastBuildDate><atom:link href="https://gquintana.github.io/rss/" rel="self" type="application/rss+xml"/><ttl>60</ttl><item><title><![CDATA[Logging configuration]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>There is nothing fundamentally new in this article.
It&#8217;s just a quick reminder (to myself) about Java logging framework configuration.</p>
</div>
<div class="paragraph">
<p>The configuration files will contain:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Console aka stdout output</p>
</li>
<li>
<p>Rolling file output with both date and size rolling policies.
Rotating the file every day is practical because it allows to find yesterday&#8217;s failure cause easily.
Rotating when the file reaches a given size allows to protect against disk flood.</p>
</li>
<li>
<p>Sample log patterns to help formating log file.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Logging libraries support multiple configuration file format: XML, Properties, YAML&#8230;&#8203; I chose to use XML, it&#8217;s a matter of taste.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_logback">Logback</h2>
<div class="sectionbody">
<div class="listingblock">
<div class="title">pom.xml</div>
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">        &lt;dependency&gt;
            &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt;
            &lt;artifactId&gt;logback-classic&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Logback implements SLF4J from the ground up.</p>
</div>
<div class="listingblock">
<div class="title">logback.xml</div>
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;configuration debug="true"&gt;<i class="conum" data-value="1"></i><b>(1)</b>
    &lt;!-- Properties --&gt;
    &lt;property name="log.dir" value="target/log" /&gt;<i class="conum" data-value="2"></i><b>(2)</b>

    &lt;!-- Appenders --&gt;
    &lt;appender name="CONSOLE" class="ch.qos.logback.core.ConsoleAppender"&gt;
        &lt;encoder&gt;<i class="conum" data-value="3"></i><b>(3)</b>
            &lt;pattern&gt;%date{HH:mm:ss.SSS} %-5level [%thread] %logger{1} - %msg%n&lt;/pattern&gt;
        &lt;/encoder&gt;
    &lt;/appender&gt;
    &lt;appender name="FILE" class="ch.qos.logback.core.rolling.RollingFileAppender"&gt;
        &lt;file&gt;blog.log&lt;/file&gt;
        &lt;rollingPolicy class="ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"&gt;
            &lt;fileNamePattern&gt;${log.dir}/blog.%d{yyyy-MM-dd}-%i.log&lt;/fileNamePattern&gt;
            &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt;
            &lt;maxHistory&gt;10&lt;/maxHistory&gt;
            &lt;totalSizeCap&gt;100MB&lt;/totalSizeCap&gt;
        &lt;/rollingPolicy&gt;
        &lt;encoder&gt;<i class="conum" data-value="4"></i><b>(4)</b>
            &lt;pattern&gt;%date{ISO8601} %-5level [%thread] %logger - %msg%n&lt;/pattern&gt;
        &lt;/encoder&gt;
    &lt;/appender&gt;

    &lt;!-- Loggers --&gt;
    &lt;logger name="com.github.gquintana.logging" level="DEBUG"/&gt;
    &lt;root level="DEBUG"&gt;
        &lt;appender-ref ref="CONSOLE"/&gt;
        &lt;appender-ref ref="FILE"/&gt;
    &lt;/root&gt;
&lt;/configuration&gt;</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The <code>debug</code> flag enables Logback startup logs.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>log.dir</code> property can be overriden at JVM (<code>-Dlog.dir=&#8230;&#8203;</code>) or OS level.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>The <code>pattern</code> is documented in the <a href="https://logback.qos.ch/manual/layouts.html#ClassicPatternLayout">layout</a> section.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Format the <code>date</code> in ISO8601.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can force Logback to use a specific configuration file using a JVM property <code>-Dlogback.configurationFile=/path/to/config.xml</code>.</p>
</div>
<div class="paragraph">
<p>The <a href="https://logback.qos.ch/manual/index.html">Logback Manual</a> contains detailed information.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_log4j2">Log4J2</h2>
<div class="sectionbody">
<div class="listingblock">
<div class="title">pom.xml</div>
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-slf4j-impl&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.apache.logging.log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j-core&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Log4J2-SLF4J adapter is in the Log4J2 group.</p>
</div>
<div class="listingblock">
<div class="title">log4j2.xml</div>
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;?xml version="1.0" encoding="UTF-8"?&gt;
&lt;Configuration status="trace"&gt;<i class="conum" data-value="1"></i><b>(1)</b>
    &lt;!-- Properties --&gt;
    &lt;Properties&gt;<i class="conum" data-value="2"></i><b>(2)</b>
        &lt;Property name="logDir"&gt;${sys:log.dir:-target/log}&lt;/Property&gt;
    &lt;/Properties&gt;

    &lt;!-- Appenders --&gt;
    &lt;Appenders&gt;
        &lt;Console name="CONSOLE"&gt;<i class="conum" data-value="3"></i><b>(3)</b>
            &lt;PatternLayout pattern="%date{HH:mm:ss.SSS} %-5level [%thread] %logger{1} - %msg%n"/&gt;
        &lt;/Console&gt;
        &lt;RollingFile name="FILE"
                     fileName="${logDir}/blog.log"
                     filePattern="${logDir}/blog.%d{yyyy-MM-dd}-%i.log.gz"&gt;
            &lt;PatternLayout&gt;<i class="conum" data-value="4"></i><b>(4)</b>
                &lt;Pattern&gt;%d{ISO8601} %-5level [%thread] %logger %m%n&lt;/Pattern&gt;
            &lt;/PatternLayout&gt;
            &lt;Policies&gt;
                &lt;TimeBasedTriggeringPolicy/&gt;
                &lt;SizeBasedTriggeringPolicy size="1m" /&gt;
            &lt;/Policies&gt;
            &lt;Strategies&gt;
                &lt;DefaultRolloverStrategy max="10"/&gt;
            &lt;/Strategies&gt;
        &lt;/RollingFile&gt;
    &lt;/Appenders&gt;

    &lt;!-- Loggers --&gt;
    &lt;Loggers&gt;
        &lt;Logger name="com.github.gquintana.logging" level="debug"/&gt;
        &lt;Root level="info"&gt;
            &lt;AppenderRef ref="CONSOLE"/&gt;
            &lt;AppenderRef ref="FILE"/&gt;
        &lt;/Root&gt;
    &lt;/Loggers&gt;
&lt;/Configuration&gt;</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Setting <code>status</code> to <code>trace</code> or <code>debug</code> shows Log4J2 internal logs.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>logDir</code> property is set from JVM property (<code>-Dlog.dir=&#8230;&#8203;</code>) with default value.
See <a href="https://logging.apache.org/log4j/2.x/manual/configuration.html#Property_Substitution">Property substitution</a> in documentation.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>The <code>PatternLayout</code> is documented in the <a href="https://logging.apache.org/log4j/2.x/manual/layouts.html#PatternLayout">layout</a> section.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Format the <code>date</code> in ISO8601.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can tell Log4J2 to load a specific configuration file using a JVM property <code>-Dlog4j.configurationFile=/path/to/config.xml</code>.</p>
</div>
<div class="paragraph">
<p>The <a href="http://logging.apache.org/log4j/2.x/manual/index.html">Log4J2 Manual</a> contains extensive documentation.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_log4j1">Log4J1</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Even if it&#8217;s deprecated, let&#8217;s end with the venerable Log4J v1 library.</p>
</div>
<div class="listingblock">
<div class="title">pom.xml</div>
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">        &lt;dependency&gt;
            &lt;groupId&gt;log4j&lt;/groupId&gt;
            &lt;artifactId&gt;log4j&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;
        &lt;dependency&gt;
            &lt;groupId&gt;org.slf4j&lt;/groupId&gt;
            &lt;artifactId&gt;slf4j-log4j12&lt;/artifactId&gt;
            &lt;scope&gt;runtime&lt;/scope&gt;
        &lt;/dependency&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Log4J1-SLF4J adapter is in the SLF4J group.</p>
</div>
<div class="listingblock">
<div class="title">log4j.xml</div>
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;
&lt;!DOCTYPE log4j:configuration SYSTEM "log4j.dtd"&gt;
&lt;log4j:configuration xmlns:log4j="http://jakarta.apache.org/log4j/" debug="true"&gt;<i class="conum" data-value="1"></i><b>(1)</b>
    <i class="conum" data-value="2"></i><b>(2)</b>
    &lt;!-- Appenders --&gt;
    &lt;appender name="CONSOLE" class="org.apache.log4j.ConsoleAppender"&gt;
        &lt;layout class="org.apache.log4j.PatternLayout"&gt;<i class="conum" data-value="3"></i><b>(3)</b>
            &lt;param name="ConversionPattern" value="%d{HH:mm:ss.SSS} %-5p %c{1} - %m%n"/&gt;
        &lt;/layout&gt;
    &lt;/appender&gt;
    &lt;appender name="FILE" class="org.apache.log4j.RollingFileAppender"&gt;<i class="conum" data-value="4"></i><b>(4)</b>
        &lt;param name="File" value="${log.dir}/blog.log"/&gt;
        &lt;param name="MaxFileSize" value="10MB"/&gt;
        &lt;param name="MaxBackupIndex" value="10"/&gt;
        &lt;layout class="org.apache.log4j.PatternLayout"&gt;<i class="conum" data-value="5"></i><b>(5)</b>
            &lt;param name="ConversionPattern" value="%d{ISO8601} %-5p [%t] %c - %m%n"/&gt;
        &lt;/layout&gt;
    &lt;/appender&gt;
    &lt;!-- Loggers --&gt;
    &lt;root&gt;
        &lt;priority value="INFO"/&gt;
        &lt;appender-ref ref="CONSOLE"/&gt;
        &lt;appender-ref ref="FILE"/&gt;
    &lt;/root&gt;
&lt;/log4j:configuration&gt;</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Like Logback, the <code>debug</code> flag enables Log4J1 internal logs.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>There aren&#8217;t any properties in Log4J1.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>The <code>PatternLayout</code> is documented in the <a href="http://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/PatternLayout.html">JavaDoc</a>.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>There is no DailyRollingFileAppender in Log4J1 unless you add <code>log4j-extras</code> extension.
Even with this extension, one can not mix size and time rollover.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Format the <code>date</code> in ISO8601.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You shouldn&#8217;t forget <code>file:/</code> if you use specific configuration file with the JVM property <code>-Dlog4j.configuration=file:///path/to/config.xml</code>.</p>
</div>
<div class="paragraph">
<p>The <a href="https://logging.apache.org/log4j/1.2/manual.html">Log4J1 Manual</a> is only a quick introduction. By the time, there was a <a href="https://www.amazon.com/Complete-Log4j-Manual-Ceki-Gulcu/dp/2970036908">book</a> .</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2019/05/17/Logging-configuration.html</link><guid isPermaLink="true">https://gquintana.github.io/2019/05/17/Logging-configuration.html</guid><category><![CDATA[java]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Fri, 17 May 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Ansible collection processing]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>As a Java developer, I sometimes dream that I can use the Java 8+ Stream API in my Ansible playbook to process list and dict variables.</p>
</div>
<div class="paragraph">
<p>In this article, I&#8217;ll show you can process a list of users:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">users:
  - id: bouh
    name: "Mary"
    admin: True
    role: child
  - id: sulli
    name: "James Sullivan"
    admin: False
    role: monster
  - id: bob
    name: "Bob Wazowski"
    admin: False
    role: assistant
  - id: celia
    name: "Celia Mae"
    admin: False
    role: assistant</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_jinja_filters">Jinja Filters</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The main tool to transform Ansible variables are Jinja filters. There are 2 libraries of filters available:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The Jinja <a href="http://jinja.pocoo.org/docs/2.10/templates/#list-of-builtin-filters">builtin filters</a>.
This list can also be found in Jinja source code <a href="https://github.com/pallets/jinja/blob/master/jinja2/filters.py">filters.py</a>.</p>
</li>
<li>
<p>The <a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_filters.html#">Ansible filters</a>
This list can also be found in Ansible source code <a href="https://github.com/ansible/ansible/tree/devel/lib/ansible/plugins/filter">filter</a> directory.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Filters are similar to Unix or Anguler pipes and can be chained.</p>
</div>
<div class="paragraph">
<p>Like in other data processing libraries there two kinds of operators:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Mappers</strong> take stream of element and produce a stream of elements: selectattr, rejectattr, map, list</p>
</li>
<li>
<p><strong>Reducers:</strong> take a stream of elements and produce a single element: join, first, last, max, min</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">admin_user_ids: |
  {{ users
  |selectattr('admin')
  |map(attribute='id')
  |join(',') }} <i class="conum" data-value="1"></i><b>(1)</b>
normal_user_count: |
  {{ users
  |rejectattr('admin')
  |list |count }} <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Take the <code>id</code> attribute of <code>users</code> having <code>admin</code> set to true and join them.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Take the <code>users</code> havin <code>admin</code> set to false and count them. As the <code>rejectattr</code> filter returns an iterator, but the <code>count</code> filter requires a list, I have to use <code>list</code> filter to convert it.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>selectattr</code>/<code>rejectattr</code> filters can take 3 arguments: the attribute, a boolean operator and an argument.
The operator can be chosen among Jinja&#8217;s <a href="http://jinja.pocoo.org/docs/2.10/templates/#list-of-builtin-tests">builtin tests</a>.
This list can also be found in Ansible source code <a href="https://github.com/pallets/jinja/blob/master/jinja2/filters.py">tests.py</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">assistant_user_ids: |
  {{ users
  |selectattr('role', 'equalto', 'assistant')
  |map(attribute='id')
  |join(',') }}</code></pre>
</div>
</div>
<div class="paragraph">
<p>With Ansible 2.7+, the <code>map</code> filter can take 3 arguments: the attribute, an operator and arguments.
The operator can be chosen among Jinja filters, and will be applied to each element of the list.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">user_first_names: |
  {{ users
  |map(attribute='name')
  |map('regex_replace', '(\\w+)( .*)?', '\\g&lt;1&gt;')
  |join(',') }} <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>For each <code>users</code> take its name and when the regular expression matches  apply the replacement, then join the result.</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_json_query">JSON Query</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Another strategy is to use a JSON Path to walk down the YAML tree.
It&#8217;s bit less verbose and bit more powerful than the previous solution.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">jq_admin_user_ids: |
  {{ users
  |json_query("[?admin].id")
  |join(',') }} <i class="conum" data-value="1"></i><b>(1)</b>
jq_assistant_user_ids: |
  {{ users
  |json_query("[?role == 'assistant'].id")
  |join(',') }} <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Take the <code>id</code> attributes of <code>users</code> having <code>admin</code> set to true and then join them.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Take the <code>id</code> attributes of <code>users</code> having <code>role</code> set to <code>assistant</code> and then join them.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This <code>json_query</code> is based on the <code>jmespath</code> Python <a href="https://pypi.org/project/jmespath/">library</a>, this means 2 things:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>You can use <a href="http://jmespath.org/">jmespath.org</a> web site to cook your JSON path query.</p>
</li>
<li>
<p>You&#8217;ll have to add the <code>jmespath</code> library to your Python environnement.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Sadly, nesting JMESPath expressions inside Jinja template expressions inside YAML files can be tricky.
This following example fails, even if the JMES path is alright in Python interpreter.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">jq_bid_user_ids: |
  {{ users
  |json_query("[?starts_with(id,'b')].id")
  |join(',') }}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>It&#8217;s possible to transform a variable containing an array into another list.
However it&#8217;s still painful to do because neither YAML nor Jinja tare programming languages.
I personnaly regret I can&#8217;t invoke Python code from Ansible playbook and use for comprehensions, imagine something like:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">py_admin_user_ids: |
  {{ ','.join([ user.id for user in users if user.admin ]) }}</code></pre>
</div>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2019/04/25/Ansible-collection-processing.html</link><guid isPermaLink="true">https://gquintana.github.io/2019/04/25/Ansible-collection-processing.html</guid><category><![CDATA[ansible]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Thu, 25 Apr 2019 00:00:00 GMT</pubDate></item><item><title><![CDATA[Structured logging with SLF4J and Logback]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>I don&#8217;t know who first coined the term <strong>structured logging</strong>.
There is <a href="https://kartar.net/2015/12/structured-logging/">a 2015 blog post by James Turnbull</a> to get started.</p>
</div>
<div class="paragraph">
<p>Python and .Net developers have libraries dedicated to structured logging : <a href="http://www.structlog.org">structlog</a> and <a href="https://serilog.net/">serilog</a>. In this article I will describe how to do structured logging in Java with usual logging libraries like SLF4J et Logback.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_structured_logging_with_slf4j">Structured logging with SLF4J</h2>
<div class="sectionbody">
<div class="paragraph">
<p>All Java developers know how to log a message:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Logger demoLogger = LoggerFactory.getLogger("logodyssey.DemoLogger");
demoLogger.info("Hello world!");</code></pre>
</div>
</div>
<div class="paragraph">
<p>Properly configured, it produces a log like</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>21:10:29.178 [Thread-1] INFO  logodyssey.DemoLogger - Hello world!</code></pre>
</div>
</div>
<div class="paragraph">
<p>Notice how this "Hello world!" message is qualified with several fields:
a timestamp, a thread Id, a level and a logger/category.</p>
</div>
<div class="paragraph">
<p>This is what the term "structured logging" means,
a log is more than a message string.
The message is associated with contextual information about what was occurring,
it tells more detail about what was going on when this log was printed</p>
</div>
<div class="paragraph">
<p>How can we enrich this contextual information provided by default,
and add the user Id for example?
It is the purpose of the MDC (Mapped Diagnostic Context):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">MDC.put("userId", "gquintana");
demoLogger.info("Hello world!");
MDC.remove("userId");</code></pre>
</div>
</div>
<div class="paragraph">
<p>The MDC is a map-like object filled in the Java code,
and used in the back-end logging library to output custom data.
With the adequate configuration, we can get the user Id the log:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>21:10:29.178 [Thread-1] gquintana INFO  logodyssey.DemoLogger - Hello world!</code></pre>
</div>
</div>
<div class="paragraph">
<p>The MDC can store any information about the user (user Id, session Id, token Id), about the current request (request Id, transaction Id), about long running threads (batch instance Id, broker client Id).
Later on, this information will be part of the log.</p>
</div>
<div class="paragraph">
<p>Having this kind of information allows to group logs by user, by request, by processing.
Remember that logs may be scattered across different servers, on different time periods.
These additional fields allow correlating logs belonging to the same scenario and finding answers to questions like "what was the user X doing when he met this nasty error?"</p>
</div>
<div class="paragraph">
<p>Let&#8217;s get back to the example, we saw the MDC stores extra information about logs.
The MDC is usually based on a thread local variable, this has two drawbacks:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>It must be properly cleaned after being used, or you may experience information leaks if the thread is reused. Think about thread pools in web servers like Tomcat.</p>
</li>
<li>
<p>The information may no be properly transfered from one thread to another. Think about asynchronous calls.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>As a result, calling <code>MDC.remove</code>, like the above example, (or <code>MDC.clear</code>) is required to clean the MDC after usage.
In order not to forget to do the housework afterwards, we can use a try-with-resource construct:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">try(MDC.MDCCloseable mdc = MDC.putCloseable("userId", "gquintana")) {
	demoLogger.info("Hello world!");
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>It&#8217;s better but still verbose.
Hopefully, this kind of code won&#8217;t make its way in your business code because it is usually hidden in an interceptor like a Servlet filter, a Spring aspect or a JAXRS interceptor. In Logback, there is a <code>MDCInsertingServletFilter</code> class which can serve as an example.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_json_logging_with_logback">JSON logging with Logback</h2>
<div class="sectionbody">
<div class="paragraph">
<p>At this point, a log is more than simple string,
it is qualified with useful information: timestamp, level, thread, user Id&#8230;&#8203;
How can we write this data structure on disk or send it over the wire to a log collection tool?
We have to serialize it.
For a human being, a simple text format as shown above is readable enough.
However, for a machine, this is just a word soup without any structure.
In short, to send structured logs to a log collection tool
and benefit from this structure (search by user, by thread&#8230;&#8203;),
we must use a structured format, like JSON for example.</p>
</div>
<div class="paragraph">
<p>Compared to the Syslog format, another popular log format, the JSON format</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Can properly handle  multi-line logs like stack traces/call traces or messages containing line separators (wanted or not)</p>
</li>
<li>
<p>Is a versatile format and can have custom fields like user Id, transaction Id</p>
</li>
<li>
<p>Is more verbose, so compression (GZip or the like) may be required to reduce the weight</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Most popular log collection tools likes Filebeat, Graylog, Fluentd already use some kind of compressed JSON format under the hood.
You should too.</p>
</div>
<div class="paragraph">
<p>Generating JSON logs with Logback is very easy.
I&#8217;ll show how to use two Logback extensions,
the <a href="https://github.com/logstash/logstash-logback-encoder">Logstash Logback encoder</a>
and the <a href="https://github.com/qos-ch/logback-contrib/wiki">Logback Contrib</a> library.</p>
</div>
<div class="paragraph">
<p>The first one uses a Logback extension point known as <strong>encoder</strong> that you can plug into any appender:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">    &lt;appender name="FILE" class="ch.qos.logback.core.FileAppender"&gt;
        &lt;file&gt;log/log-odyssey.log&lt;/file&gt;
        &lt;encoder class="net.logstash.logback.encoder.LogstashEncoder"&gt;
            &lt;customFields&gt;{"application":"log-odyssey"}&lt;/customFields&gt;
        &lt;/encoder&gt;
    &lt;/appender&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>It will produce the expected result:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
  "@timestamp": "2017-11-25T21:10:29.178+01:00",
  "@version": 1,
  "message": "Hello world!",
  "logger_name": "logodyssey.DemoLogger",
  "thread_name": "Thread-1",
  "level": "INFO",
  "level_value": 20000,
  "HOSTNAME": "my-laptop",
  "userId": "gquintana",
  "application": "log-odyssey"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Maven coordinates for this library are <code>net.logstash.logback:logstash-logback-encoder:4.11</code>.</p>
</div>
<div class="paragraph">
<p>The second one uses a different extension point called <strong>layout</strong>.
In the end, it looks very similar to the first one, a bit more verbose though:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">    &lt;appender name="FILE" class="ch.qos.logback.core.FileAppender"&gt;
        &lt;file&gt;log/log-odyssey.log&lt;/file&gt;
        &lt;encoder class="ch.qos.logback.core.encoder.LayoutWrappingEncoder"&gt;
            &lt;layout class="ch.qos.logback.contrib.json.classic.JsonLayout"&gt;
                &lt;jsonFormatter class="ch.qos.logback.contrib.jackson.JacksonJsonFormatter"/&gt;
                &lt;appendLineSeparator&gt;true&lt;/appendLineSeparator&gt;
            &lt;/layout&gt;
        &lt;/encoder&gt;
    &lt;/appender&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result is very close as well, even though the fields are named differently:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{
    "timestamp":"1511814391083",
    "level":"INFO",
    "thread":"Thread-1",
    "mdc": {
        "userId":"gquintana"
	},
    "logger":"logodyssey.DemoLogger",
    "message":"Hello world!",
    "context":"default"
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>In order to be on par with the first example, it is possible to subclass the <code>JsonLayout</code> and add custom fields:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public class CustomJsonLayout extends JsonLayout {
    @Override
    protected void addCustomDataToJsonMap(Map&lt;String, Object&gt; map, ILoggingEvent event) {
        map.put("application", "log-odyssey");
        try {
            map.put("host", InetAddress.getLocalHost().getHostName());
        } catch (UnknownHostException e) {
        }
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Several Maven dependencies are required <code>ch.qos.logback.contrib:logback-json-classic:0.1.5</code>,
<code>ch.qos.logback.contrib:logback-jackson:0.1.5</code> and <code>com.fasterxml.jackson.core:jackson-databind</code>
for this library to work.</p>
</div>
<div class="paragraph">
<p>In the end these libraries are similar, both use the Jackson library to generate JSON.
Contrary to the above JSON examples which have been prettyfied to be human readable, producing one JSON document per line is better because it is more compact, and each end of line marks the end of a log, there is no multi-line log.
This format is known as <a href="http://ndjson.org/">NDJSON</a> or and <a href="http://jsonlines.org/">JSON Lines</a>.
Logstash and Filebeat can easily read this kind of JSON file.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A log is more than a textual message, it can be enriched with information at different levels:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Line of code: message, timestamp, level, threadId, appender&#8230;&#8203;</p>
</li>
<li>
<p>User or transaction: user Id, session Id&#8230;&#8203;</p>
</li>
<li>
<p>Deployment unit: application Id, container Id, host Id, environment Id (production, staging)&#8230;&#8203;</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Once qualified with this contextual information,
the log message becomes a structured piece of information
and must be processed as such.
Producing logs in JSON format allows to keep that structure
and eases storing these logs in Elasticsearch.
More on that later, it time permits.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2017/12/01/Structured-logging-with-SL-FJ-and-Logback.html</link><guid isPermaLink="true">https://gquintana.github.io/2017/12/01/Structured-logging-with-SL-FJ-and-Logback.html</guid><category><![CDATA[java]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Fri, 01 Dec 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[Log collection in AWS land]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In AWS, it is really easy to popup new machines and scale.
The more machines you have, the more important it&#8217;s to centralize logs.
In this article, I will describe what I discovered while trying to collect logs from applications deployed on Beanstalk and send them into Elasticsearch.</p>
</div>
<div class="paragraph">
<p>Disclaimer: I am an AWS newbie.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_family_picture">The family picture</h2>
<div class="sectionbody">
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2017-09-30-Log-collection-in-AWS-land/big-picture.svg" alt="Big picture">
</div>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1">Beanstalk</dt>
<dd>
<p>contains a web server and runs an application (Java, JS, Go&#8230;&#8203;), both produce logs in a local <code>/var/log/something</code> directory.
There can be multiple instances of the same application for scalability, or it can be different applications in the same environment.</p>
</dd>
<dt class="hdlist1">Cloudwatch</dt>
<dd>
<p>is used to monitor EC2, Beanstalk&#8230;&#8203; instances, it is the place where logs and metrics are gathered.
From there you can trigger alerts, schedule tasks&#8230;&#8203;</p>
</dd>
<dt class="hdlist1">S3</dt>
<dd>
<p>is a file storage where can be used to archive logs on long term and survive instances stop.
However theses logs are not easily searchable because they are compressed files.</p>
</dd>
<dt class="hdlist1">Elasticsearch</dt>
<dd>
<p>can be used to index logs and make them searchable.
A Kibana UI is provided to make search and dashboard building even more easy.</p>
</dd>
<dt class="hdlist1">Lambda</dt>
<dd>
<p>a provided Lambda function is used to bridge logs from Cloudwatch to Elasticsearch</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>To ship logs into Cloudwatch, an AWSLogs agent is provided.
To archive logs into S3, a script is cron-ed along with logrotate.</p>
</div>
<div class="paragraph">
<p>This article will skip the security (IAM) settings which are required to allow these components to communicate.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_beanstalk">Beanstalk</h2>
<div class="sectionbody">
<div class="paragraph">
<p>There are several components producing logs in a Beanstalk instance:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Beanstalk deployment</p>
</li>
<li>
<p>Proxy server: either Apache or Nginx produce Access logs and Error logs</p>
</li>
<li>
<p>Web server: Tomcat, NodeJS&#8230;&#8203;</p>
</li>
<li>
<p>Application with its own logging framework</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>Each component produces logs with its own format.
Beanstalk knows how to automatically collect logs for the first three components: deploy logs, access logs, web server logs&#8230;&#8203;
As it knows where this logs are located it is in charge of rotating, archiving on S3, and purging log files.</p>
</div>
<div class="paragraph">
<p>The Beanstalk console allows to <a href="http://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.logging.html">download 100</a> lines of each file for a given instance.
It is useful to understand why a deployment fails, but it&#8217;s not meant to dig into the logs of a running application cluster.</p>
</div>
<div class="paragraph">
<p>To tell Beanstalk to take care of your application specific log files, just at some configuration files to indicate where they are located:</p>
</div>
<div class="listingblock">
<div class="title">/opt/elasticbeanstalk/tasks/taillogs.d/my-app.conf</div>
<div class="content">
<pre class="highlight"><code>/var/log/my-app/my-app.log</code></pre>
</div>
</div>
<div class="listingblock">
<div class="title">/opt/elasticbeanstalk/tasks/bundlelogs.d/my-app.conf</div>
<div class="content">
<pre class="highlight"><code>/var/log/my-app/my-app.*.log</code></pre>
</div>
</div>
<div class="paragraph">
<p>The first file allows <code>my-app.log</code> (current log file) content to appear in the Beanstalk console.
The second file allows all (old log files) files to be archived.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cloudwatch_logs">Cloudwatch Logs</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First of all, the log stream from Beanstalk to Cloudwatch must be enabled in Beanstalk configuration file:</p>
</div>
<div class="listingblock">
<div class="title">&nbsp;.ebextensions/cloudwatch.config</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">option_settings:
  aws:elasticbeanstalk:cloudwatch:logs:
    StreamLogs: true
    DeleteOnTerminate: false
    RetentionInDays: 30</code></pre>
</div>
</div>
<div class="paragraph">
<p>This sets up the Cloudwatch log agent.
Provided you&#8217;re using an image based on Amazon Linux, you can also install it on any EC2 instance with yum:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">yum update -y
yum install -y awslogs
service awslogs start</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then the Cloudwatch log agent must be configured to watch your custom log files.</p>
</div>
<div class="listingblock">
<div class="title">/etc/awslogs/config/my-app.conf</div>
<div class="content">
<pre class="highlight"><code class="language-toml" data-lang="toml">[/var/log/my-app/my-app.log]
log_group_name=/aws/elasticbeanstalk/my-app-dev/var/log/my-app/my-app.log
log_stream_name={instance_id}
file=/var/log/my-app/my-app*.log</code></pre>
</div>
</div>
<div class="paragraph">
<p>Log files produced my components (Apache, Tomcat&#8230;&#8203;) managed by Beanstalk are already configured.
The above config file is only for application specific log files.
This agent support multiline logs (like stacktraces, call trace&#8230;&#8203;) provided they start with a whitespace (space, tab).
It&#8217;s not as powerful as Filebeat or Logstash.</p>
</div>
<div class="paragraph">
<p>At this point, you&#8217;ll be able to see logs aggregated from multiple Beanstalk instances in the Cloudwatch console.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2017-09-30-Log-collection-in-AWS-land/cloudwatch_log_search.png" alt="Cloudwatch log viewer">
</div>
</div>
<div class="paragraph">
<p>It&#8217;s better than Beanstalk console to monitor a running platform.
Yet, log search is still limited because logs are not structured (split into fields) and the full text search is simplistic.</p>
</div>
<div class="paragraph">
<p>Using Cloudwatch, it is also possible :</p>
</div>
<div class="ulist">
<ul>
<li>
<p>to raise alerts when a specific pattern is found in logs</p>
</li>
<li>
<p>to extract metrics from logs (HTTP request time, number of 404 errors in Access logs for example) and draw charts</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>More information can be found <a href="https://aws.amazon.com/fr/blogs/aws/cloudwatch-log-service/">here</a>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_elasticsearch">Elasticsearch</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To send logs into Elasticsearch and get a better log search experience,
subscribe a log filter to each Cloudwatch log group.
There is a special Lambda which can do Log filtering and send logs to Elasticsearch.
This log filter can be used to split text logs into fields:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2017-09-30-Log-collection-in-AWS-land/cloudwatch_log_filter.png" alt="Cloudwatch log filter">
</div>
</div>
<div class="paragraph">
<p>This tool can split space delimited logs, like access logs, into fields.
But it&#8217;s hard to "grok" more complicated logs with such basic tool.
It supports JSON formatted logs, so a good solution for application logs, is to configure your favorite logging framework to produce JSON logs.</p>
</div>
<div class="paragraph">
<p><a href="https://medium.com/wolox-driving-innovation/centralized-logging-in-microservices-using-aws-cloudwatch-elasticsearch-f5db7a57e553">This article</a> is worth reading.</p>
</div>
<div class="paragraph">
<p>At this point, we can open Kibana and configure an index pattern named <code>cwl-*</code>.
Cloudwatch log filter mimics Logstash and uses a field named <code>@timestamp</code> for timestamp</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>AWS provides all the building blocks to centralize logs and monitor your whole infrastructure.
It&#8217;s not hard to collect logs and send them in Elasticsearch.
But it&#8217;s also far less powerful than the complete Elastic stack.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2017/09/30/Log-collection-in-AWS-land.html</link><guid isPermaLink="true">https://gquintana.github.io/2017/09/30/Log-collection-in-AWS-land.html</guid><category><![CDATA[elasticsearch]]></category><category><![CDATA[cloud]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Sat, 30 Sep 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[Java File vs Path]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>I&#8217;ve been using <code>java.io.File</code> and <code>java.io.File*Stream</code> since Java 1.1, a long time ago.
Java 7 introduced a new file API named <strong>NIO2</strong> containing, among others, the <code>java.nio.file.Path</code> and <code>java.nio.file.Files</code> classes.
It took me a while to lose my habits and embrace the new API.</p>
</div>
<div class="paragraph">
<p>Spoiler: The most funny part of this article is at the end!</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_quick_comparison">Quick comparison</h2>
<div class="sectionbody">
<table class="tableblock frame-all grid-all spread">
<colgroup>
<col style="width: 50%;">
<col style="width: 50%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">java.io.File (class)</th>
<th class="tableblock halign-left valign-top">java.nio.file.Path (interface)</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file = new File("path/to/file.txt")</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>path = Paths.get("path/to/file.txt")</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file = new File(parentFile, "file.txt")</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>path = parentPath.resolve("file.txt")</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file.getFileName()</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>path.getFileName().toString()</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file.getParentFile()</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>path.getParent()</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file.mkdirs()</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Files.createDirectories(path)</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file.length()</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Files.size(path)</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file.exists()</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Files.exists(path)</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file.delete()</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Files.delete(path)</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>new FileOutputStream(file)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Files.newOutputStream(path)</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>new FileInputStream(file)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Files.newInputStream(path)</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>file.listFiles(filter)</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>Files.list(path) .filter(filter) .collect(toList())</code></p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Some additional notes:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>Path</code> throws <code>IOException</code> more often than <code>File</code>, and rarely return a <code>boolean</code> to tell if something was done (<code>mkdirs()</code>, <code>delete()</code>)</p>
</li>
<li>
<p><code>File</code> is more object oriented than <code>Path</code>: I regret that <code>size()</code>, <code>exists()</code>&#8230;&#8203; methods are not on the <code>Path</code> interface. This is probably due to the fact that this API was added in Java 7, but default methods on interfaces were added later in Java 8.</p>
</li>
<li>
<p><code>Path</code> based <code>InputStream</code>/`OutputStream`s are less expensive from a GC point view. Thanks <a href="https://twitter.com/thekittster/status/905326864251670532">@kittster</a> for mentionning <a href="https://www.cloudbees.com/blog/fileinputstream-fileoutputstream-considered-harmful">this article from Cloudbees</a>.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_one_liners">One liners</h2>
<div class="sectionbody">
<div class="paragraph">
<p><code>java.nio.file.Files</code> allows to read, write, copy files in a single line:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Files.write(Paths.get("image.png"), bytes); <i class="conum" data-value="1"></i><b>(1)</b>

List&lt;String&gt; lines = Files.readAllLines(Paths.get("letter.txt"), StandardCharsets.UTF_8); <i class="conum" data-value="2"></i><b>(2)</b>

Files.lines(Paths.get("letter.txt"), StandardCharsets.UTF_8)
		.forEach(System.out::println);</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Write a binary file</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Read a text file</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This nearly makes Guava IO and Commons IO  useless. I regret that there isn&#8217;t any method out of the box to read/write a whole file as a single string.</p>
</div>
<div class="paragraph">
<p>Many APIs (JAXB, Jackson to name a few) don&#8217;t use <code>Path`s to read/write files, the workaround is usually use an `InputStream</code> or an <code>OutputStream</code>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">try(InputStream inputStream = Files.newInputStream(path)) {
  Thing thing = (Thing) unmarshaller.unmarshal(inputStream);
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_multiple_file_systems">Multiple file systems</h2>
<div class="sectionbody">
<div class="paragraph">
<p>When the <code>File</code> is only for local files, <code>Path</code> can also be used to access remote files.
A <code>Path</code> is associated to a <code>FileSystem</code>.</p>
</div>
<div class="paragraph">
<p>To create a new <code>Path</code> instances, there is not constructor (<code>Path</code> is interface), we need to call a factory method. The above 2 lines are the same:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">path = Paths.get("path/to/file.txt");
path = FileSystems.getDefault().getPath("path/to/file.txt");</code></pre>
</div>
</div>
<div class="paragraph">
<p>As the default file system is the local one, you get a path to a local file.
Depending on the underlying file system, you&#8217;ll get a different implementation: <code>sun.nio.fs.UnixPath</code>, <code>sun.nio.fs.WindowsPath</code>&#8230;&#8203;</p>
</div>
<div class="paragraph">
<p>With this trick in mind, we can read the content of a Zip file, as if we had extracted it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">URI zipUri = new URI("jar:file:/path/to/archive.zip")
try(FileSystem zipFS = FileSystems.newFileSystem(zipUri, emptyMap())) { <i class="conum" data-value="1"></i><b>(1)</b>
    Path zipPath = zipFS.getPath("/archive") <i class="conum" data-value="2"></i><b>(2)</b>
	Files.list(zipPath)
		.map(Path::toString)
		.forEach(System.out::println);
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>"Mount" the Zip file as a file system</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td><code>zipPath</code> is of type <code>com.sun.nio.zipfs.ZipPath</code></td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You can even plug additional file systems: <a href="http://docs.oracle.com/javase/7/docs/technotes/guides/io/fsp/zipfilesystemprovider.html">ZIP</a>, <a href="https://github.com/maddingo/nio-fs-provider">SFTP, SMB, WebDAV</a>, <a href="https://github.com/lucastheisen/jsch-nio">SSH/SCP</a>, <a href="https://github.com/Upplication/Amazon-S3-FileSystem-NIO2">Amazon S3</a>, <a href="https://github.com/google/jimfs">In memory</a>, <a href="https://github.com/damiencarol/jsr203-hadoop">HDFS</a>, &#8230;&#8203;
This almost means you can read a remote file as if it were local.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2017/09/02/Java-File-vs-Path.html</link><guid isPermaLink="true">https://gquintana.github.io/2017/09/02/Java-File-vs-Path.html</guid><category><![CDATA[java]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Sat, 02 Sep 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[Ansible and rolling upgrades]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Ansible is a nice tool to deploy distributed systems like Elasticsearch, Kafka, Cassandra and the like.
These systems are built with high availability in mind and can tolerate partial failures.
However, upgrading these softwares, or updating their configuration, requires restarting each member of the cluster.
Care must be taken when deploying changes to avoid complete unavailability.</p>
</div>
<div class="paragraph">
<p>The aim of this article is to describe a pattern I discovered
trying to improve the deployment speed of Ansible playbooks
and yet being able to guarantee availability during this upgrade process.
This is the story of two contradicting goals.
To improve deployment speed, you need to deploy all hosts in parallel.
To guarantee availability, you can&#8217;t stop all hosts at the same time: you must deploy each host one after the other.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_problem">The problem</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s take the Elasticsearch example to explain Ansible basics.</p>
</div>
<div class="paragraph">
<p>The tasks required to deploy Elasticsearch on each host of the cluster are gathered in an <code>elasticsearch</code> role.
In short, the <code>elasticsearch</code> role is the recipe to install and update an Elasticsearch on a given machine.
Once declared, this role is applied to all hosts belonging to the <code>elasticsearch</code> group in the Ansible playbook.</p>
</div>
<div class="listingblock">
<div class="title">playbook.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch</code></pre>
</div>
</div>
<div class="paragraph">
<p>At some point, the <code>elasticsearch</code> role will probably contain something to stop Elasticsearch in order for changes to be reloaded:</p>
</div>
<div class="listingblock">
<div class="title">roles/elasticsearch/tasks/main.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">...
- name: Stop service
  become: true
  service:
    name: elasticsearch
    state: stopped
    enabled: true
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>This naive Ansible playbook will stop all Elasticsearch nodes at the same time,
making the whole cluster unavailable until enough nodes are started again.
To fix this problem, the role can be ran in serial instead of parallel:</p>
</div>
<div class="listingblock">
<div class="title">deploy.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">- hosts: es
  serial: 1 # Force serial execution
  roles:
    - role: elasticsearch</code></pre>
</div>
</div>
<div class="paragraph">
<p>But deploying every host, one after the other (in serial), takes a very long time and makes deployment endless.
Bigger clusters mean longer deployment times.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_refactoring_the_role">Refactoring the role</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The main idea to improve speed is to split the role into multiple steps, and do as much as possible in parallel:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>In Parallel</strong>: Deploy system settings, software settings, software binaries.
But don&#8217;t stop anything and don&#8217;t remove anything at this point.</p>
</li>
<li>
<p><strong>In Serial</strong>:</p>
<div class="ulist">
<ul>
<li>
<p>Stop the node</p>
</li>
<li>
<p>Install or upgrade the node as quickly as possible, do as few things as possible.</p>
</li>
<li>
<p>Start the node</p>
</li>
</ul>
</div>
</li>
<li>
<p><strong>In Parallel</strong>: finish applying settings on the running cluster and remove old files and binaries.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>For this purpose, describe each step in its own YAML file:
<code>before.yml</code> for step 1, <code>stop_start.yml</code> for step 2 and <code>after.yml</code> for step 3.
Finally, the role entry point <code>main.yml</code> will include the appropriate step using a variable named <code>elasticsearch_step</code>:</p>
</div>
<div class="listingblock">
<div class="title">roles/elasticsearch/tasks/main.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">- name: "Running step {{ elasticsearch_step }}"
  include: "{{ elasticsearch_step }}.yml"</code></pre>
</div>
</div>
<div class="paragraph">
<p>This little trick, will allow calling the <code>elasticsearch</code> role, yet running only a part of it.</p>
</div>
<div class="paragraph">
<p>We can also create a fake step including all the steps, it will allow to run the whole role as before (more on that later):</p>
</div>
<div class="listingblock">
<div class="title">roles/elasticsearch/tasks/all.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">- include: "before.yml"
- include: "stop_start.yml"
- include: "after.yml"</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_refactoring_the_playbook">Refactoring the playbook</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now, in the playbook we will call the role 3 times, each individual step being called independently.</p>
</div>
<div class="listingblock">
<div class="title">playbook.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># In parallel
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
      elasticsearch_step: "before"

# In serial
- hosts: elasticsearch
  any_errors_fatal: true
  serial: 1
  roles:
    - role: elasticsearch
      elasticsearch_step: "stop_start"

# In parallel
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
      elasticsearch_step: "after"</code></pre>
</div>
</div>
<div class="paragraph">
<p>To install a brand new cluster, there is nothing to stop, and we don&#8217;t fear anything.
In this particular case, this role can still be used, and the <code>stop_start</code> step can be called in parallel:</p>
</div>
<div class="listingblock">
<div class="title">playbook.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># New cluster
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
      elasticsearch_step: "before"
    - role: elasticsearch
      elasticsearch_step: "stop_start"
    - role: elasticsearch
      elasticsearch_step: "after"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Or even simpler, we can use the fake <code>all</code> step:</p>
</div>
<div class="listingblock">
<div class="title">playbook.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># New cluster
- hosts: elasticsearch
  any_errors_fatal: true
  roles:
    - role: elasticsearch
      elasticsearch_step: "all"</code></pre>
</div>
</div>
<div class="paragraph">
<p>You may have noticed the <code>serial</code> attribute is a number, I set to 1.
For big clusters, and provided you have more than one replica of your data,
you can stop&#8217;n&#8217;start nodes two by two, three by three&#8230;&#8203;</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_unreloaded_configuration">Unreloaded configuration</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Most of the time, I am only running the Ansible playbook to change settings that don&#8217;t need nodes to be restarted.
To skip the expensive part, the trick is to detect in the <code>before</code> step whether nodes should be restarted or not.
A a result, the <code>before</code> step should mark whether the <code>stop_start</code> is required:</p>
</div>
<div class="listingblock">
<div class="title">roles/elasticsearch/tasks/before.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">- set_fact:
    elasticsearch_restart_needed: True
  when: ...</code></pre>
</div>
</div>
<div class="paragraph">
<p>On lucky days, you can skip the expensive <code>stop_start</code> step and have a quick and fully parallel deployment.</p>
</div>
<div class="paragraph">
<p>Other days, when upgrading software version, or changing configuration which can not be hot reloaded, running the Ansible playbook will be slower.
Node specific configuration (<code>elasticsearch.yml</code>, Kafka <code>server.properties</code>&#8230;&#8203;) is usually part of the problem as it requires node restart.</p>
</div>
<div class="listingblock">
<div class="title">playbook.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">- hosts: elasticsearch
  any_errors_fatal: true
  serial: 1
  roles:
    - role: elasticsearch
      elasticsearch_step: "stop_start"
  when: elasticsearch_restart_needed defined and elasticsearch_restart_needed</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_cluster_wide_configuration">Cluster wide configuration</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In distributed systems, some configuration must be done only once for the whole cluster.
Here are some examples:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Elasticsearch</strong>: License, Users and grants, Indices, Mappings, Templates, Cluster settings (allocation awareness, minimum master nodes&#8230;&#8203;)</p>
</li>
<li>
<p><strong>Kafka</strong>: Users and grants, Topics</p>
</li>
<li>
<p><strong>Cassandra</strong>: Users and grants, Keyspaces, Tables</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This kind of configuration must be done on a running cluster, the cluster will probably replicate the change using internal specific mechanisms.
Obviously, it must be ran in the <code>after</code> step, once the cluster is started and listening.</p>
</div>
<div class="listingblock">
<div class="title">roles/elasticsearch/tasks/after.yml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml"># Configure article index
- uri:
    url: "http://{{ ansible_ssh_hostname }}:9200/article"
    method: PUT
    body_format: json
    body: "{{ lookup('file','article_setting.json') }}"
  run_once: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>The trick here is to use <code>run_once</code> to play this task on a single node.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2017/07/15/Ansible-and-rolling-upgrades.html</link><guid isPermaLink="true">https://gquintana.github.io/2017/07/15/Ansible-and-rolling-upgrades.html</guid><category><![CDATA[ansible]]></category><category><![CDATA[elasticsearch]]></category><category><![CDATA[kafka]]></category><category><![CDATA[cassandra]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Sat, 15 Jul 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[From Java to Go]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This is the second episode of my <em>7 languages in 7 weeks</em> series of blog posts.
After <a href="2017-01-08-From-Java-to-Ruby.html">Ruby</a>, a language which is more than 20 years old,
I tried a more recent language, Go which was created in 2009.</p>
</div>
<div class="paragraph">
<p>Go gained in traction thanks to the Docker ecosystem:
Docker container engine, Kubernetes, <a href="https://traefik.io/">Traefik</a> are mostly developed in Go.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_good">The good</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_simple_to_learn">Simple to learn</h3>
<div class="paragraph">
<p>Even if the language is pretty recent, there are many resources to learn Go.
The first one I stumbled upon was <a href="https://tour.golang.org/">Tour of Go</a>,
it allows to discover the Go features and test them online.
You can even run the web server locally if you want to learn Go while traveling,
it has even been translated in many languages. In a word, awesome!</p>
</div>
<div class="paragraph">
<p>I also tried the <a href="https://github.com/cdarwin/go-koans">Go Koans</a>,
but the tests are less detailed and progressive than Ruby Koans.
It took me an evening to complete these exercises.</p>
</div>
<div class="paragraph">
<p>There are several free e-books to get started with Go:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="http://www.golang-book.com/books/intro">Introduction to Programming in Go</a> by Caleb Doxsey</p>
</li>
<li>
<p><a href="http://openmymind.net/The-Little-Go-Book/">The Little Go Book</a> by Karl Seguin</p>
</li>
<li>
<p><a href="http://www.golangbootcamp.com/">Go BootCamp</a> by Matt Aimonetti</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Go is mostly simple to grasp because there are few concepts.
The Go Lang spec is less 100 pages long.</p>
</div>
<div class="paragraph">
<p>Obviously the "Go" name is not SEO friendly, you&#8217;d better google "GoLang" instead.</p>
</div>
</div>
<div class="sect2">
<h3 id="_rich_tooling">Rich tooling</h3>
<div class="paragraph">
<p>As soon as you&#8217;ve installed Go and without any extra step, you can:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Get</strong> libraries (Go packages) with a sort of package manager</p>
</li>
<li>
<p><strong>Format</strong> code to stick to Go formatting rules</p>
</li>
<li>
<p><strong>Compile</strong> code and link dependencies.
I was amazed to see how fast it was to compile a simple app and run it.</p>
</li>
<li>
<p><strong>Run</strong> unit tests.
Go provides a unit testing package, sadly it&#8217;s not as powerful as others XUnit libraries.</p>
</li>
<li>
<p><strong>Analyze</strong> code and detect suspicious constructs.</p>
</li>
<li>
<p><strong>Search and Read</strong> the base API documentation while being offline.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Sadly, there is no <a href="https://github.com/blindpirate/report-of-go-package-management-tool">official tool</a> to manage project dependencies
and download them with Go Get.</p>
</div>
</div>
<div class="sect2">
<h3 id="__code_switch_code_statement"><code>switch</code> statement</h3>
<div class="paragraph">
<p>Compare Go&#8217;s switch statement</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">	switch i {
	case 0:
		fmt.Print("None")
	case 1:
		fmt.Print("Single ")
		fallthrough
	default:
		fmt.Print("Thing")
	}</code></pre>
</div>
</div>
<div class="paragraph">
<p>With Java&#8217;s</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">    switch i {
    case 0:
      System.out.print("None");
      break;
    case 1:
      System.out.print("Single ")
    default:
      System.out.println("Thing")
    }</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>fallthrough</code> keyword does the exact opposite of <code>break</code>.
This looks like a better default than Java to me,
because most of the time you don&#8217;t want to run multiple cases.
A missing <code>break</code> can raise subtle bugs.</p>
</div>
<div class="paragraph">
<p>This <code>switch</code> statement, can also replace a bunch of <code>if</code>/<code>else if</code> statements:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">	switch {
	case i == 0:
		fmt.Print("None")
	case i == 1:
		fmt.Print("One ")
	case i &gt; 1:
		fmt.Print("Many ")
	default:
		fmt.Print("Invalid")
	}</code></pre>
</div>
</div>
<div class="paragraph">
<p>This looks like pattern matching, but simpler and less powerful.</p>
</div>
</div>
<div class="sect2">
<h3 id="_concurrency_with_channels_and_routines">Concurrency with channels and routines</h3>
<div class="paragraph">
<p>First of all, a <strong>channel</strong> is a concept used for inter-thread communication.
There are 2 types of channels:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Sync channels</strong> are similar to Java&#8217;s <code>SynchronousQueue</code> ,
the producer waits for the consumer to be ready:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">ch := make(chan string)</code></pre>
</div>
</div>
</li>
<li>
<p><strong>Async channels</strong> are similar to Java&#8217;s <code>ArrayBlockingQueue</code> ,
the producer waits for a free location to place data,
while the consumer waits for a place filled with data:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">ch := make(chan string, 10)</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Goroutines</strong> are lightweight threads, to run the <code>doit</code> function in background:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">go doit()</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the following example, a thread will generate and send numbers to another thread.
The receiver thread will multiply the numbers and print them:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2017-01-15-From-Java-to-Go/concurrency.svg" alt="Channels and Goroutines">
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">func generate(out chan [2]int) {
	for x := 1; x &lt;= 10; x++ {
		for y := 1; y &lt;= 10; y++ {
			t := [2]int{x, y}
			fmt.Printf("generate %dx%d\n", t[0], t[1])
			out &lt;- t                                    <i class="conum" data-value="1"></i><b>(1)</b>
		}
	}
	fmt.Println("generate end")
}

func multiply(in chan [2]int) {
	for {
		t := &lt;-in                                           <i class="conum" data-value="2"></i><b>(2)</b>
		r := [3]int{t[0], t[1], t[0] * t[1]}
		fmt.Printf("multiply %dx%d=%d\n", r[0], r[1], r[2])
	}
}

func main() {
	in := make(chan [2]int, 10)
	go multiply(in)                                             <i class="conum" data-value="3"></i><b>(3)</b>
	generate(in)                                                <i class="conum" data-value="4"></i><b>(4)</b>
	time.Sleep(10 * time.Second)
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Generate and append data in the channel. This function will run in a first thread.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Pull data from the channel and process it. This function will run in a second thread.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Run the <code>multiply</code> function in a background thread</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Run the <code>generate</code> function in the main thread.
I could have placed it in a separate thread as well.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This multi-threading pattern, called CSP (communicating sequential processes) looks bit like Actor model (Akka and the like).
An actor is more or less an async channel with a goroute polling from this channel, like the <code>multiply</code> function.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_bad">The bad</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_error_handling">Error handling</h3>
<div class="paragraph">
<p>There has been <a href="http://openmymind.net/Golangs-Error-Handling-Good-And-Bad/">debate</a> about how errors are handled.
I hope not to start a new troll with this paragraph.</p>
</div>
<div class="paragraph">
<p>Prior to explaining my grief with error handling, let me talk about multiple values.
A function can return multiple values, however there are no tuples like in Python, Scala&#8230;&#8203;</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">file, err := ioutil.ReadFile("file.txt") // Correct
tuple :=     ioutil.ReadFile("file.txt") // Incorrect</code></pre>
</div>
</div>
<div class="paragraph">
<p>I can not assign both values to a single variable.</p>
</div>
<div class="paragraph">
<p>Now, let&#8217;s come back to error handling. In Go there is no Exception concept, you must use multiple return values:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">func Order(quantity int) (*Article, error) {                <i class="conum" data-value="1"></i><b>(1)</b>
	if quantity &lt;= 0 {
		return nil, errors.New("Invalid quantity")  <i class="conum" data-value="2"></i><b>(2)</b>
	}
	return &amp;Article{quantity, 10 * quantity}, nil       <i class="conum" data-value="3"></i><b>(3)</b>
}
func OrderAndPrint(quantity int) error {
	article, err := Order(quantity)
	if err != nil {
		return err                                  <i class="conum" data-value="4"></i><b>(4)</b>
	}
	article.Print()                                     <i class="conum" data-value="5"></i><b>(5)</b>
	return nil
}
func main() {
	err := OrderAndPrint(-3)
	if err != nil {
		fmt.Errorf("Error %s", err)                 <i class="conum" data-value="6"></i><b>(6)</b>
	}
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Function can raise an error</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Raise an error</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Return normal result</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Propagate the error</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Handle normal flow</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>Handle error flow</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>To me, this means multiple things</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In the <code>Order</code> function, I must <strong>always</strong> return 2 things: <code>nil+error</code> (1) or <code>result+nil</code> (2).
I can even return both <code>result+error</code>.</p>
</li>
<li>
<p>I need to <strong>manually</strong> propagate the error when it occurs (3).
Yet the function signature warns me that I may have to do something,
and I can&#8217;t hardly forget to deal with it.</p>
</li>
<li>
<p>Even if I forward the error, the error doesn&#8217;t contain <strong>any call stack</strong>.
As a result, it&#8217;s probably harder to debug since you don&#8217;t know at first sight who
first returned the error (5).
Hopefully the <a href="https://github.com/go-errors/error">go-errors</a> library may help.</p>
</li>
<li>
<p>I can not easily chain function calls
because I can not write <code>Order(3).Print()</code> or <code>Print(Order(3))</code> where <code>Print</code> is a function to display the result of <code>Order</code>.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_pointers">Pointers</h3>
<div class="paragraph">
<p>Pointers are not a bad thing per se.
In my case, C/C++ lectures were far away in my memory.
It was not easy to remember the traps and tricks:
Should I pass this variable by reference or by value?</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">func RaisePriceByVal(a Article) {
	a.Price = (a.Price * 110) / 100
}

func RaisePriceByRef(a *Article) {
	a.Price = (a.Price * 110) / 100                           <i class="conum" data-value="1"></i><b>(1)</b>
}

func newArticle(name string, price int) *Article {
	a := Article{name, price}
	return &amp;a                                                 <i class="conum" data-value="5"></i><b>(5)</b>
}

func main() {
	a := Article{"Go in Action", 25}
	fmt.Printf("Article %s %d\n", a.Name, a.Price)
	RaisePriceByVal(a)
	fmt.Printf("By val: Article %s %d\n", a.Name, a.Price)    <i class="conum" data-value="2"></i><b>(2)</b>
	RaisePriceByRef(&amp;a)                                       <i class="conum" data-value="3"></i><b>(3)</b>
	fmt.Printf("By ref: Article %s %d\n", a.Name, a.Price)    <i class="conum" data-value="4"></i><b>(4)</b>
	p := newArticle("The Little Go book", 10)                 <i class="conum" data-value="6"></i><b>(6)</b>
	RaisePriceByRef(p)
	fmt.Printf("By ref: Article %s %d\n", p.Name, p.Price)
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Happily, I don&#8217;t have to convert the pointer into value to access the <code>Price</code> field.
I don&#8217;t have to write <code>*a.Price</code> or <code>a&#8594;Price</code> like in C++</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The article price is still 25 because it was passed by value</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Here I have to convert the value into a pointer</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>The article price is now 27 because it was passed by reference</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Unlike C, the function can return a pointer to the created article</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td><code>p</code> is a pointer to the created article</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Having to deal with pointers, I felt like doing a step backward.</p>
</div>
</div>
<div class="sect2">
<h3 id="__code_make_code_code_new_code_and_code_new_code"><code>make</code>, <code>new</code> and <code>New</code></h3>
<div class="paragraph">
<p>To create, I mean allocate, something there are several operators depending on the type
and whether you are expecting a pointer or not.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">	// Structs
	struct1 := Language{"go", "Go Lang"}                      <i class="conum" data-value="1"></i><b>(1)</b>
	fmt.Println("struct1 ", struct1)
	struct2 := new(Language)                                  <i class="conum" data-value="2"></i><b>(2)</b>
	fmt.Println("struct2 ", struct2)

	// Arrays
	array1 := [5]Language{struct1, *struct2, Language{"java", "Java"}} <i class="conum" data-value="3"></i><b>(3)</b>
	fmt.Println("array1 ", array1)
	array2 := new([5]Language)                                <i class="conum" data-value="4"></i><b>(4)</b>
	fmt.Println("array2 ", array2)

	// Slices
	slice1 := array1[1:3]
	fmt.Println("slice1 ", slice1)
	slice2 := make([]Language, 1, 5)                          <i class="conum" data-value="5"></i><b>(5)</b>
	fmt.Println("slice2 ", slice2)

	// Maps
	map1 := map[string]Language{                              <i class="conum" data-value="6"></i><b>(6)</b>
		"go":   struct1,
		"java": Language{"java", "Java"},
	}
	fmt.Println("map1 ", map1)
	map2 := make(map[string]Language, 3)                      <i class="conum" data-value="7"></i><b>(7)</b>
	fmt.Println("map2 ", map2)

	// Other
	error1 := errors.New("Sample error")                      <i class="conum" data-value="8"></i><b>(8)</b>
	fmt.Println("error1 ", error1)</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Create a new struct and initialize its fields</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Create a new struct and return a pointer</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Create a new array and initialize its elements</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Create a new array and return an pointer</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Create a new slice</td>
</tr>
<tr>
<td><i class="conum" data-value="6"></i><b>6</b></td>
<td>Create a new map and initialize its elements</td>
</tr>
<tr>
<td><i class="conum" data-value="7"></i><b>7</b></td>
<td>Create a new map</td>
</tr>
<tr>
<td><i class="conum" data-value="8"></i><b>8</b></td>
<td>Create a new struct <code>Error</code> and initialize it</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>new</code> operator always returns a pointer, it&#8217;s like a <code>malloc</code> operator in C.
The <code>make</code> operator is used to create data structures like slices, maps&#8230;&#8203;
Neither the <code>new</code> nor the <code>make</code> operators can be passed values,
they are initialized with zeros and empty strings.
The <code>New</code> function is a factory, it can contain code to initialize or build something.</p>
</div>
<div class="paragraph">
<p>I find it a bit disturbing to have different syntaxes to do mostly the same thing.
For a language which aims at simplicity, this is baffling.</p>
</div>
<div class="paragraph">
<p>Most of the time, when creating a struct, you will initialize its fields
and get a pointer to avoid copying the value.
As a result, the main syntax is:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">struct := &amp;Language{"go", "Go Lang"}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_harsh_compiler">Harsh compiler</h3>
<div class="paragraph">
<p>The Go compiler is very strict, this can be good thing at times as it may prevent bugs.
But it can also be a bit too picky and annoying at times.
In particular, it doesn&#8217;t accept unused variables and unused imports.
This is a rather common scenario when you are refactoring, looking for the cleanest code design or trying to implement an idea.
The compiler could just warn and skip unused declarations.</p>
</div>
<div class="paragraph">
<p>To skip a useless import, one can write (notice the underscore):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">import _ "fmt"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Using an IDE (like Jetbrains Gogland) with an intelligent coding assistant
which would fix automatically imports may help.
I also found the <a href="https://godoc.org/golang.org/x/tools/cmd/goimports">goimports</a> command line tool,
but I didn&#8217;t try it.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_odd">The odd</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Characters are called <strong>Runes</strong> in the Go terminology.
But they don&#8217;t hide any secret magic ;-) .</p>
</div>
<div class="sect2">
<h3 id="__code_if_code_and_code_for_code_statements"><code>if</code> and <code>for</code> statements</h3>
<div class="paragraph">
<p>The <code>for</code> statement uses the usual syntax minus the parenthesis:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">	for i := 0; i &lt; 10; i++ {</code></pre>
</div>
</div>
<div class="paragraph">
<p>Like in the above <code>for</code> construct, the <code>if</code> can declare a variable before the mandatory condition:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">		if dice := rand.Int31n(6) + 1; dice &gt; 4 {
			fmt.Printf("%d You won!", dice)
		} else {
			fmt.Printf("%d You lost!", dice)
		}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>dice</code> variable is visible in the while <code>if</code>/<code>else</code> bloc.
I find the <code>if</code> statement to be less readable because
the <code>dice</code> declaration adds noise, and the condition doesn&#8217;t catch my eye.</p>
</div>
<div class="paragraph">
<p>There is no <code>while</code> statement, <code>for</code> is the only loop keyword</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">	var line = "start"
	reader := bufio.NewReader(os.Stdin)
	for line != "quit" {                       <i class="conum" data-value="1"></i><b>(1)</b>
		fmt.Println(line)
		line, _ = reader.ReadString('\n')
		line = strings.TrimSpace(line)     <i class="conum" data-value="2"></i><b>(2)</b>
	}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The <code>for</code> with a condition acts as like the <code>while</code> in most languages</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>I can not chain <code>ReadString</code> and <code>TrimSpace</code> functions in single line
because the read can return an error I should have handled.
This is the problem I explained with error handling and method chaining.</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_the_code_code_symbol">The <code>:=</code> symbol</h3>
<div class="paragraph">
<p>The <code>:=</code> symbol is called <em>short variable declaration</em>,
it triggers type inference so you don&#8217;t have to declare the type of your local variable.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">var i int = 12
i := 12</code></pre>
</div>
</div>
<div class="paragraph">
<p>Both variable declaration are the same.</p>
</div>
<div class="paragraph">
<p>I just wonder why there is a special symbol for that, even if I found it very efficient in practice.
At first, I thought it was to tell apart assignment from comparison (<code>==</code>) like in Pascal.</p>
</div>
</div>
<div class="sect2">
<h3 id="_packages_and_imports">Packages and imports</h3>
<div class="paragraph">
<p>I wonder why imported packages are strings wrapped by double quotes:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">import "fmt"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Like Java, Go&#8217;s package naming convention reflects domain names and folder paths:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">import "github.com/go-errors/errors"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Moreover, when there are subpackages, only the last part of the package is used in the code:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">import 	"io/ioutil"                                        <i class="conum" data-value="1"></i><b>(1)</b>

func main() {
	data, err := ioutil.ReadFile("subpackage.go")      <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Notice that the imported package is named <code>io/ioutil</code> not <code>io/util</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>To use a function in this package, I use <code>ioutil.Readfile</code> and skip the <code>io</code> prefix.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Naming a package <code>util</code>( or <code>common</code> or &#8230;&#8203;) in Go is probably a poor idea.
For instance <code>string/util</code> would be conflicting with <code>date/util</code>,
unless you give an alias to the import, but it looks less convenient:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">import stringutil "string/util"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The Go compiler doesn&#8217;t allow circular dependencies in packages which sounds like a good idea.</p>
</div>
</div>
<div class="sect2">
<h3 id="_structs_and_interfaces">Structs and interfaces</h3>
<div class="paragraph">
<p>Go doesn&#8217;t have classes, inheritance and OOP concepts, but it doesn&#8217;t matter,
structs are powerful and can have interfaces.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-go" data-lang="go">type Person struct {
	Name string
	Age  int
}

// NewPerson Constructor of Person
func NewPerson(name string) *Person {              <i class="conum" data-value="1"></i><b>(1)</b>
	return &amp;Person{name, 0}
}

// GetName Function on Person
func (p *Person) GetName() string {                <i class="conum" data-value="1"></i><b>(1)</b>
	return p.Name
}

// Nameable interface
type Nameable interface {                          <i class="conum" data-value="2"></i><b>(2)</b>
	GetName() string
}

// SayName Function taking an interface as argument
func SayName(n Nameable) {                         <i class="conum" data-value="3"></i><b>(3)</b>
	fmt.Printf("My name is %s\n", n.GetName())
}

func main() {
	p := NewPerson("John")
	p.Age = 12
	SayName(p)                                 <i class="conum" data-value="4"></i><b>(4)</b>
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>I wonder why the <code>NewPerson</code> constructor and the <code>GetName</code> function can not declared inside the struct.
In the <code>GetName</code> function, the Go Linter warns that the <code>Person</code> argument shouldn&#8217;t be named <code>this</code> or <code>self</code>, this is why I named it <code>p</code>.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Notice the <code>Person</code> struct doesn&#8217;t tell it implements the <code>Nameable</code> interface.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>A function should never take a pointer to an interface,
because an interface is already a <a href="https://research.swtch.com/interfaces">kind of pointer</a> delegating to the original struct.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>The variable <code>p</code> of type <code>*Person</code> is automatically converted into <code>Nameable</code> interface.
Go is doing duck typing, if it has a <code>GetName</code> function, then it is a <code>Nameable</code> thing.</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To me, the Go language is well suited for low level tools
(container orchestrators, databases, proxies&#8230;&#8203;), command line utilities
as well as embedded and mobile applications.
But it is not has expressive as many other languages for business applications.</p>
</div>
<div class="paragraph">
<p>I don’t pretend to be a Go expert at all, if I wrote something wrong, tell me.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2017/01/15/From-Java-to-Go.html</link><guid isPermaLink="true">https://gquintana.github.io/2017/01/15/From-Java-to-Go.html</guid><category><![CDATA[7li7w]]></category><category><![CDATA[go]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Sun, 15 Jan 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[From Java to Ruby]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>One of my 2017 resolutions is to try different languages and see how they compare to Java.
This is inspired by the <a href="https://pragprog.com/book/btlang/seven-languages-in-seven-weeks">7 languages in 7 weeks</a> book by Bruce A. Tate.
I don&#8217;t expect to learn a new language each week though, since it took me several weeks only for the first one.</p>
</div>
<div class="paragraph">
<p>The first language I&#8217;ve tried is Ruby.
I&#8217;ve already seen Ruby used many times: Vagrant configuration, Logstash plugins, FluentD plugins, web applications like Kibana 2&#8230;&#8203;
But I don&#8217;t have any experience in Ruby development.
So I used the <a href="http://rubykoans.com/">Ruby Koans</a> to teach myself Ruby.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_good">The good</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_syntactic_sugar_everywhere">Syntactic sugar everywhere</h3>
<div class="paragraph">
<p>Ruby has a lot of syntactic sugar which makes code really easy to read, and short to write.</p>
</div>
<div class="paragraph">
<p>The first example deals with arrays:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">a=["one","two","three","four"]
puts a[1..-2] <i class="conum" data-value="1"></i><b>(1)</b>

first, *tail = a <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Prints two and three whose indices are between 1 and length-2</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The array is destructured: <code>first</code> contains one and <code>tail</code> an array of two, three, four</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The second example shows the templating system in strings:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">i=4
puts "Square root of #{i} is #{Math.sqrt(i)}" <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Prints <em>Square root of 4 is 2.0</em></td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Now let&#8217;s see getters and setters.
By default attributes a private, while methods are public:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">class Person
  def name            <i class="conum" data-value="1"></i><b>(1)</b>
    @name
  end
  def name=(new_name) <i class="conum" data-value="2"></i><b>(2)</b>
    @name=new_name
  end
end

p=Person.new
p.name="John Doe"     <i class="conum" data-value="3"></i><b>(3)</b>
puts p.name           <i class="conum" data-value="4"></i><b>(4)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Declare a getter, <code>return</code> is optional</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Declare a setter, notice this funny method name <code>name=</code></td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Call the setter</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Call the getter</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>I&#8217;ll show later that getters and setters can be generated.</p>
</div>
</div>
<div class="sect2">
<h3 id="_meta_programming">Meta programming</h3>
<div class="paragraph">
<p>Ruby has a powerful API similar to Java reflection:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">class Talkative
  def say_hello
    puts "Hello"
  end
  def method_missing(method_name, *arguments, &amp;block)
    puts "Call #{method_name} with #{arguments}" <i class="conum" data-value="1"></i><b>(1)</b>
  end
end

s = Talkative.new
s.say_hello
s.say("Goodbye")</code></pre>
</div>
</div>
<div class="paragraph">
<p>As the <code>say</code> method is not implemented, the <code>method_missing</code> is called.
You will notice that a method call is made of a <code>method_name</code>, an array of <code>arguments</code>, and a code <code>block</code> (I&#8217;ll tell more on that later).</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Hello
Call say with ["Goodbye"]</pre>
</div>
</div>
<div class="paragraph">
<p>The above example shows how to generate methods in an existing class.
With this API, creating a proxy to wrap an object and do some kind of AOP is simple:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">class Proxy
  def initialize(target_object)
    @object = target_object
  end
  def method_missing(method_name, *arguments, &amp;block)
    if @object.respond_to?(method_name)                      <i class="conum" data-value="1"></i><b>(1)</b>
      puts "Before #{method_name} #{arguments}"
      result = @object.send(method_name, *arguments, &amp;block) <i class="conum" data-value="2"></i><b>(2)</b>
      puts "After #{method_name}: #{result}"
      return result
    end
  end
end

s = Proxy.new("Hello world!")                                <i class="conum" data-value="3"></i><b>(3)</b>
puts s.upcase</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Check whether the target object has the method or not</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Call the method on the target object.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Wrap the string in a proxy</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>Before upcase []
After upcase: HELLO WORLD!
HELLO WORLD!</pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_bad">The bad</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_strings">Strings</h3>
<div class="paragraph">
<p>Strings are mutable:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">hi = "Hello "
hi &lt;&lt; "world"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The above example is similar to a Java <code>StringBuilder</code>: at the end <code>hi</code> contains <code>Hello world</code>.
I know that having immutable Strings in Java allows many optimizations in the JVM, I wonder what&#8217;s the performance penalty of having mutable strings.</p>
</div>
<div class="paragraph">
<p>Multiline strings is something I miss in Java, but Ruby&#8217;s syntax is rather strange.
They are delimited with <code>%{ &#8230;&#8203; }</code> or <code>%! &#8230;&#8203; !</code> or &#8230;&#8203;:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">multiline = %{
First line
Second line
}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_unless">Unless</h3>
<div class="paragraph">
<p>I am not a big fan of the <code>unless</code> keyword which does the contrary of <code>if</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">rude = false
unless rude    <i class="conum" data-value="1"></i><b>(1)</b>
  puts "Please"
end</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Is the same as <code>if !rude</code></td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Sticking to the <code>if</code> keyword looks like better option to me.</p>
</div>
<div class="paragraph">
<p>However, I have to admit that this syntax is not as bad:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">rude = true
child.say_hello("mister") unless rude</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_functional_programming">Functional programming</h3>
<div class="paragraph">
<p>Doing functional programming with Ruby is clearly possible.
But it doesn&#8217;t look natural to me.</p>
</div>
<div class="paragraph">
<p>First the lambda functions syntax could be simpler:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">twice = lambda { |x| 2 * x } <i class="conum" data-value="1"></i><b>(1)</b>
puts twice.call(3) <i class="conum" data-value="2"></i><b>(2)</b>

twice = lambda do |x| <i class="conum" data-value="3"></i><b>(3)</b>
  2 * x
end</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Means x &#8594; 2 * x</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>call</code> method is used to invoke the function</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>the <code>{ &#8230;&#8203; }</code> delimiter can be replaced by <code>do &#8230;&#8203; end</code> for multiline functions</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Then, passing functions to methods uses a special feature.
Methods can receive a block of code as a special argument,
this block is then called using the <code>yield</code> keyword.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">class Demo
  def say_hello
    if block_given?
      message = yield("Hello World!") <i class="conum" data-value="2"></i><b>(2)</b>
    else
      message = "Hello World!"
    end
    puts message
  end
  def yell_hello
    say_hello { |s| s.upcase } <i class="conum" data-value="1"></i><b>(1)</b>
  end
end</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Invoke the <code>say_hello</code> method if a block of code</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Invoke the block and pass it some parameters</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>I first wondered why the block wasn&#8217;t passed as other method arguments,
and why it was handled as a special argument.
The reason is probably that it allows do "sandwich code":</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">open(file_name) do |file| <i class="conum" data-value="1"></i><b>(1)</b>
  # Do something with opened file
end <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Open the file named <code>file_name</code> and make it available to the block.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Close the file</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The <code>open</code> method takes two arguments the file name and the code block dealing with the file.
In the end, this looks like Java&#8217;s try-with-resources statement.</p>
</div>
</div>
<div class="sect2">
<h3 id="_open_classes">Open classes</h3>
<div class="paragraph">
<p>Classes are open which means, anyone can add a method to any class, even system ones:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">class ::Integer      <i class="conum" data-value="1"></i><b>(1)</b>
  def even?          <i class="conum" data-value="2"></i><b>(2)</b>
    (self % 2) == 0
  end
end

puts 1.even?         <i class="conum" data-value="3"></i><b>(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Let&#8217;s add a method to integers</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Funny fact: in Ruby&#8217;s naming conventions method returning a boolean should end with <code>?</code></td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Prints <code>false</code></td>
</tr>
</table>
</div>
<div class="paragraph">
<p>While it can be useful, placed in wrong hands, this weapon may be harmful.
Yet existing methods can not be replaced.</p>
</div>
</div>
<div class="sect2">
<h3 id="_building_and_packaging">Building and packaging</h3>
<div class="paragraph">
<p>Ruby libraries and applications are packaged as <em>Gems</em> which are Zip archives
containing code and metadata, a bit like Java&#8217;s Jar files.</p>
</div>
<div class="paragraph">
<p>To start a Ruby project, 4 tools are needed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Ruby</strong>: there are multiple versions and variants (MRI, JRuby&#8230;&#8203;) to choose from.
<strong><a href="https://rvm.io">RVM</a></strong> (Ruby Version Manager) is an optional tool to install
and activate the appropriate one.</p>
</li>
<li>
<p><strong>Gem</strong> is the tool to download gems from <a href="http://rubygems.org" class="bare">http://rubygems.org</a>
(which is a sort of Maven Central repository)
and unzip them in the local repository.
It&#8217;s a package manager like Yum, NPM&#8230;&#8203;</p>
</li>
<li>
<p><strong><a href="http://bundler.io">Bundler</a></strong> takes a bill of materials,
the list of dependencies for the project,
and runs Gem to download and install them.</p>
</li>
<li>
<p><strong><a href="http://www.rubyrake.org">Rake</a></strong>
(Ruby Make) is Ruby build tool, it can process files, run unit tests&#8230;&#8203;</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>And 2 files are need to describe a project:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Gemfile</strong> contains the list of Gems needed to build and run the project. It&#8217;s used by Bundler.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>source 'https://rubygems.org'

gem 'rspec'
gem 'rspec-collection_matchers'</pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Rakefile</strong> describes how to build the project. It&#8217;s Rake configuration file.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>begin
  require 'rspec/core/rake_task'

  RSpec::Core::RakeTask.new(:spec)

  task :default =&gt; :spec
rescue LoadError
  # RSpec not available
end</pre>
</div>
</div>
<div class="paragraph">
<p>For the newcomer, starting a project is not a trivial task.
I had to read each tool documentation independently,
look for an example of each config file and assemble the whole.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_the_odd">The odd</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Objects have a numeric Id which can be used for reference comparison:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">o=Object.new
puts o.object_id <i class="conum" data-value="1"></i><b>(1)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The object Id is a numeric value, for instance 4376500</td>
</tr>
</table>
</div>
<div class="sect2">
<h3 id="_symbols">Symbols</h3>
<div class="paragraph">
<p>I had hard time understanding the concept of <strong>Symbol</strong>.</p>
</div>
<div class="paragraph">
<p>A symbol is a kind of immutable and unique string which can be used as a constant or as a label.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">color = :red
color = :blue</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this example, <code>red</code> and <code>blue</code> are symbols used as enum values.</p>
</div>
<div class="paragraph">
<p>Symbols are used with meta programming to identify methods, attributes and so forth:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">class Person
  attr_reader :id  <i class="conum" data-value="1"></i><b>(1)</b>
  attr_accessor :name <i class="conum" data-value="2"></i><b>(2)</b>
end</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The <code>id</code> attribute is read-only, it has a setter</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>name</code> attribute is read/write, it has a getter and a setter</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Symbols are also used as hash map keys because they are unique:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">ages = { :bob =&gt; 23, :john =&gt; 35 }
ages = { bob: 23, john: 35}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Since Ruby 2, the two above hashes are identical.</p>
</div>
</div>
<div class="sect2">
<h3 id="_modules">Modules</h3>
<div class="paragraph">
<p>Modules are a bit like Java packages, they are used as namespaces.</p>
</div>
<div class="paragraph">
<p>The weird part, is that a module can contain classes and a class can contain modules:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">module Demo <i class="conum" data-value="1"></i><b>(1)</b>
  class Thing
    module Nameable <i class="conum" data-value="2"></i><b>(2)</b>
      def set_name(name)
        @name = name
      end
    end
  end
  class Pet &lt; Thing
    include Thing::Nameable
  end
end

medor = Demo::Pet.new
medor.set_name("Médor") <i class="conum" data-value="3"></i><b>(3)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>The <code>Demo</code> module is used as a <em>container</em> for <code>Thing</code> and <code>Pet</code> classes.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>The <code>Nameable</code> module is used as a <em>mixin</em>.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>The <code>Pet</code> class as <code>set_name</code> method.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In fact a module and a class are close concepts, a class is a module which can be instiated.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_conclusion">Conclusion</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Ruby is close to Java because it has the same OOP roots: both are inspired by Smalltalk,
both were born in the nineties.</p>
</div>
<div class="paragraph">
<p>I don&#8217;t pretend to be a Ruby expert at all, if I wrote something wrong, tell me.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2017/01/08/From-Java-to-Ruby.html</link><guid isPermaLink="true">https://gquintana.github.io/2017/01/08/From-Java-to-Ruby.html</guid><category><![CDATA[7li7w]]></category><category><![CDATA[ruby]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Sun, 08 Jan 2017 00:00:00 GMT</pubDate></item><item><title><![CDATA[Testing a Java and Elasticsearch 5.0 application]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>A long time ago, I wrote <a href="https://blog.zenika.com/2013/04/29/integrer-elasticsearch-dans-une-application-java/">this article in french</a> explaining
how to test a Java application talking to Elasticsearch.
At that time (Elasticsearch 1.x), it was easy to start en embedded Elasticsearch, it was a oneliner:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Node node = NodeBuilder.nodeBuilder().node();</code></pre>
</div>
</div>
<div class="paragraph">
<p>Starting with Elasticsearch 5.0, it&#8217;s <a href="https://www.elastic.co/blog/elasticsearch-the-server#_embedded_elasticsearch_not_supported">forbidden to start an embedded Elasticsearch</a>.
The <code>NodeBuilder</code> class used above doesn&#8217;t exist anymore, and most of classes used to start Elasticsearch as a server are hidden and sealed.
In short, it&#8217;s not as easy as before to test your Elasticsearch access layer, Java developers have lost their advantage of using the same language as Elasticsearch.
Most solutions described in this article can probably apply to other languages: Ruby, Python, JavaScript&#8230;&#8203;</p>
</div>
<div class="paragraph">
<p>Let&#8217;s see which options are left.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_elasticsearch_testing_framework">Elasticsearch Testing framework</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This test framework is used by the Elastic team to test Elasticsearch itself.
Undercover, it uses the <a href="http://labs.carrotsearch.com/randomizedtesting.html">Randomized Testing</a> which is also used to test Apache Lucene.
This library can be downloaded as a Maven dependency:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;dependency&gt;
  &lt;groupId&gt;org.elasticsearch.test&lt;/groupId&gt;
  &lt;artifactId&gt;framework&lt;/artifactId&gt;
  &lt;version&gt;5.0.1&lt;/version&gt;
  &lt;scope&gt;test&lt;/scope&gt;
&lt;/dependency&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>It contains an <code>ESIntegTestCase</code> which is able to bootstrap an Elasticsearch for tests:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">public class CustomerDaoTests extends ESIntegTestCase {

}</code></pre>
</div>
</div>
<div class="paragraph">
<p>It looks simple and convenient, but for the average Java developer, this library raises multiple problems:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It brings numerous dependencies in your test environment (Log4J2, Commons *&#8230;&#8203;)</p>
</li>
<li>
<p>It doesn&#8217;t mix properly with usual libraries (see jar hell error below)</p>
</li>
<li>
<p>It enforces security by enabling Java&#8217;s Security Manager (see access denied error below)</p>
</li>
<li>
<p>It doesn&#8217;t support HTTP protocol (only internal transport procol)</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>java.lang.RuntimeException: found jar hell in test classpath

	at org.elasticsearch.bootstrap.BootstrapForTesting.&lt;clinit&gt;(BootstrapForTesting.java:90)
	at org.elasticsearch.test.ESTestCase.&lt;clinit&gt;(ESTestCase.java:145)


java.security.AccessControlException: access denied ("java.lang.RuntimePermission" "accessDeclaredMembers")

	at java.security.AccessControlContext.checkPermission(AccessControlContext.java:472)</pre>
</div>
</div>
<div class="paragraph">
<p>In short, this library is appropriate to develop an Elasticsearch plugin, but that&#8217;s all.</p>
</div>
<div class="paragraph">
<p>If we can not start Elasticsearch from inside the test, let&#8217;s start it from outside.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_elasticsearch_scripting">Elasticsearch scripting</h2>
<div class="sectionbody">
<div class="paragraph">
<p>From now on, we will start a real Elasticsearch process,
we won&#8217;t use anymore a slimmed down version with specifics settings.
Our tests will get more realistic, but also harder to set up.</p>
</div>
<div class="paragraph">
<p>In order to automate test execution, my first solution is to create a script to install and start Elasticsearch.
This script can later be ran from CI configuration, before running the build script (Maven, Gradle&#8230;&#8203;).</p>
</div>
<div class="paragraph">
<p>Here is a short example:</p>
</div>
<div class="listingblock">
<div class="title">elasticsearch-start.sh</div>
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">#!/usr/bin/env bash

TARGET_DIR="$(dirname $0)/target"
ES_VERSION=5.0.0
ES_DIR=${TARGET_DIR}/elasticsearch-${ES_VERSION}
mkdir -p ${TARGET_DIR}

ES_TAR=${TARGET_DIR}/elasticsearch-${ES_VERSION}.tar.gz
ES_URL=https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-${ES_VERSION}.tar.gz
curl -o ${ES_TAR} ${ES_URL}                                                     <i class="conum" data-value="1"></i><b>(1)</b>
tar -xzf ${ES_TAR} -C ${TARGET_DIR}                                             <i class="conum" data-value="2"></i><b>(2)</b>

cd ${ES_DIR}
bin/elasticsearch -d -p pid                                                     <i class="conum" data-value="3"></i><b>(3)</b>

sleep 10s
curl "http://localhost:9200/_cluster/health?wait_for_status=yellow&amp;timeout=30s" <i class="conum" data-value="4"></i><b>(4)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Download Elasticsearch. We could also download it from a corporate repository (Nexus, web server&#8230;&#8203;).</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Unzip the downloaded archive.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Start Elasticsearch as a background process and keep it&#8217;s PID so as to be able to stop it at the end.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Wait for Elasticsearch to be up and running.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>The main drawback of this script is that it can not run on Windows: it is not portable.
We could rewrite it using Ant, <a href="https://github.com/elastic/elasticsearch/blob/5.0/dev-tools/smoke_test_rc.py">Python</a> or Groovy/Gradle:</p>
</div>
<div class="listingblock">
<div class="title">build.xml</div>
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">&lt;target name="elasticsearch-start" &gt;
    &lt;get src="https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-${es.version}.tar.gz"
         dest="${target.dir}/elasticsearch-${es.version}.tar.gz"
         verbose="true" skipexisting="true"
    /&gt;
    &lt;untar src="${target.dir}/elasticsearch-${es.version}.tar.gz" dest="${target.dir}" compression="gzip"/&gt;
    &lt;exec executable="cmd" failonerror="true" osfamily="winnt" dir="${es.dir}"&gt;
        &lt;arg value="/c"/&gt;
        &lt;arg value="bin/elasticsearch.bat"/&gt;
    &lt;/exec&gt;
    &lt;exec executable="sh" failonerror="true" osfamily="unix" dir="${es.dir}"&gt;
        &lt;arg value="bin/elasticsearch"/&gt;
        &lt;arg value="-d"/&gt;
        &lt;arg value="-p"/&gt;
        &lt;arg value="pid"/&gt;
    &lt;/exec&gt;
    &lt;waitfor maxwait="30" maxwaitunit="second" checkevery="10" checkeveryunit="second"&gt;
        &lt;http url="http://localhost:9200/_cluster/health?wait_for_status=yellow&amp;amp;timeout=5s"/&gt;
    &lt;/waitfor&gt;
&lt;/target&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>This Ant script does exactly the same as the above shell script (download, unzip, start and wait).
I have to admit, this idea is not mine, it&#8217;s explained in detail on <a href="http://david.pilato.fr/blog/2016/10/18/elasticsearch-real-integration-tests-updated-for-ga/">David Pilato&#8217;s blog</a>.
This Ant script could be called from Maven using the Antrun plugin or from Groovy/Gradle using <a href="http://docs.groovy-lang.org/latest/html/documentation/ant-builder.html">AntBuilder</a>.</p>
</div>
<div class="paragraph">
<p>I&#8217;ve created a <a href="https://github.com/gquintana/gquintana.github.io/tree/master/sources/2016-11-29-Testing-a-Java-and-Elasticsearch-50-application">sample Java project</a> using Elasticsearch Java Rest client to access read and write data.
Maven starts Elasticsearch, runs integration tests and finally stops Elasticsearch:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">        &lt;profile&gt;
            &lt;id&gt;ant&lt;/id&gt;
            &lt;properties&gt;
                &lt;elasticsearch.url&gt;http://localhost:9200&lt;/elasticsearch.url&gt;
            &lt;/properties&gt;
            &lt;build&gt;
                &lt;plugins&gt;
                    &lt;plugin&gt;
                        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
                        &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;
                        &lt;version&gt;2.10&lt;/version&gt;
                        &lt;executions&gt;
                            &lt;execution&gt;
                                &lt;id&gt;get-es&lt;/id&gt;
                                &lt;phase&gt;pre-integration-test&lt;/phase&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;copy&lt;/goal&gt;
                                &lt;/goals&gt;
                                &lt;configuration&gt;
                                    &lt;artifactItems&gt;
                                        &lt;artifactItem&gt;
                                            &lt;groupId&gt;org.elasticsearch.distribution.zip&lt;/groupId&gt;
                                            &lt;artifactId&gt;elasticsearch&lt;/artifactId&gt;
                                            &lt;version&gt;${elasticsearch.version}&lt;/version&gt;
                                            &lt;type&gt;zip&lt;/type&gt;
                                        &lt;/artifactItem&gt;
                                    &lt;/artifactItems&gt;
                                    &lt;outputDirectory&gt;${project.build.directory}/it&lt;/outputDirectory&gt;
                                &lt;/configuration&gt;
                            &lt;/execution&gt;
                        &lt;/executions&gt;
                    &lt;/plugin&gt;
                    &lt;plugin&gt;
                        &lt;artifactId&gt;maven-antrun-plugin&lt;/artifactId&gt;
                        &lt;version&gt;1.8&lt;/version&gt;
                        &lt;executions&gt;
                            &lt;execution&gt;
                                &lt;id&gt;start-es&lt;/id&gt;
                                &lt;phase&gt;pre-integration-test&lt;/phase&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;run&lt;/goal&gt;
                                &lt;/goals&gt;
                                &lt;configuration&gt;
                                    &lt;target&gt;
                                        &lt;property name="target.dir" value="${project.build.directory}"/&gt;
                                        &lt;property name="elasticsearch.version" value="${elasticsearch.version}"/&gt;
                                        &lt;ant antfile="${basedir}/it.xml" target="start"/&gt;
                                    &lt;/target&gt;
                                &lt;/configuration&gt;
                            &lt;/execution&gt;
                            &lt;execution&gt;
                                &lt;id&gt;stop-es&lt;/id&gt;
                                &lt;phase&gt;post-integration-test&lt;/phase&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;run&lt;/goal&gt;
                                &lt;/goals&gt;
                                &lt;configuration&gt;
                                    &lt;target&gt;
                                        &lt;property name="target.dir" value="${project.build.directory}"/&gt;
                                        &lt;property name="elasticsearch.version" value="${elasticsearch.version}"/&gt;
                                        &lt;ant antfile="${basedir}/it.xml" target="stop"/&gt;
                                    &lt;/target&gt;
                                &lt;/configuration&gt;
                            &lt;/execution&gt;
                        &lt;/executions&gt;
                    &lt;/plugin&gt;
                    &lt;plugin&gt;
                        &lt;artifactId&gt;maven-failsafe-plugin&lt;/artifactId&gt;
                        &lt;version&gt;2.19.1&lt;/version&gt;
                        &lt;executions&gt;
                            &lt;execution&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;integration-test&lt;/goal&gt;
                                    &lt;goal&gt;verify&lt;/goal&gt;
                                &lt;/goals&gt;
                            &lt;/execution&gt;
                        &lt;/executions&gt;
                    &lt;/plugin&gt;
                &lt;/plugins&gt;
            &lt;/build&gt;
        &lt;/profile&gt;</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">$ mvn -Pant install

------------------------------------------------------------------------
Building Testing with Elasticsearch 5.0 0.0.1-SNAPSHOT
------------------------------------------------------------------------

...

--- maven-antrun-plugin:1.8:run (start-es) @ test-elasticsearch5 ---
Executing tasks

main:

setup:

start:
     [echo] Starting Elasticsearch 5.0.1
     [echo] Started Elasticsearch with PID 6256
Executed tasks

--- maven-failsafe-plugin:2.19.1:integration-test (default) @ test-elasticsearch5 ---

-------------------------------------------------------
 T E S T S
-------------------------------------------------------
Running com.github.gquintana.elasticsearch.ProductRepositoryIT
Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 10.141 sec - in com.github.gquintana.elasticsearch.ProductRepositoryIT

Results :

Tests run: 4, Failures: 0, Errors: 0, Skipped: 0


--- maven-antrun-plugin:1.8:run (stop-es) @ test-elasticsearch5 ---
Executing tasks

main:

stop:
     [echo] Stopping Elasticsearch with PID 6256
     [echo] Stopped Elasticsearch
Executed tasks
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>This solution based on scripting has several shortcomings:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It should be improved to avoid downloading and decompressing the archive again and again.</p>
</li>
<li>
<p>It should be completed with a script to stop Elasticsearch and do household chores (remove, data, logs).</p>
</li>
<li>
<p>It could become more complicated: it is sometimes needed to install plugins,
tweak the <code>elasticsearch.yml</code> configuration file or set some environment variables.</p>
</li>
<li>
<p>As any piece of code, it should be loved and maintained.</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_elasticsearch_in_a_container">Elasticsearch in a container</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We can delegate the downloading, starting and stopping logic to Docker.
Starting Elasticsearch in a container is nearly as easy as:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-shell" data-lang="shell">docker run -d --name elasticsearch-5.0 -v /usr/share/elasticsearch/data:$PWD/target/data -P elasticsearch:5.0.1
curl "http://172.17.0.1:9200/_cluster/health?wait_for_status=yellow&amp;timeout=30s"

# Run tests here...

docker stop elasticsearch-5.0</code></pre>
</div>
</div>
<div class="paragraph">
<p>We should still wait for Elasticsearch to be started before running tests.
Once you&#8217;re familiar with Docker, you can run the Elasticsearch container using:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="https://plugins.jenkins.io/docker-plugin">Docker Jenkins plugin</a></p>
</li>
<li>
<p><a href="https://github.com/fabric8io/docker-maven-plugin">Docker Maven plugin</a>:</p>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-xml" data-lang="xml">        &lt;profile&gt;
            &lt;id&gt;docker&lt;/id&gt;
            &lt;properties&gt;
                &lt;elasticsearch.url&gt;http://172.17.0.1:9200&lt;/elasticsearch.url&gt;
            &lt;/properties&gt;
            &lt;build&gt;
                &lt;plugins&gt;
                    &lt;plugin&gt;
                        &lt;groupId&gt;io.fabric8&lt;/groupId&gt;
                        &lt;artifactId&gt;docker-maven-plugin&lt;/artifactId&gt;
                        &lt;version&gt;0.18.1&lt;/version&gt;
                        &lt;configuration&gt;
                            &lt;images&gt;
                                &lt;image&gt;
                                    &lt;alias&gt;elasticsearch5&lt;/alias&gt;
                                    &lt;name&gt;elasticsearch:5.0.1&lt;/name&gt;
                                    &lt;run&gt;
                                        &lt;volumes&gt;
                                            &lt;bind&gt;
                                                &lt;volume&gt;/usr/share/elasticsearch/data:${project.build.directory}/data&lt;/volume&gt;
                                            &lt;/bind&gt;
                                        &lt;/volumes&gt;
                                        &lt;env&gt;
                                            &lt;ES_JAVA_OPTS&gt;-Xmx1g -Xms1g&lt;/ES_JAVA_OPTS&gt;
                                        &lt;/env&gt;
                                        &lt;ports&gt;
                                            &lt;port&gt;9200:9200&lt;/port&gt;
                                        &lt;/ports&gt;
                                        &lt;wait&gt;
                                            &lt;http&gt;
                                                &lt;url&gt;${elasticsearch.url}/_cluster/health?wait_for_status=yellow&amp;amp;timeout=30s&lt;/url&gt;
                                                &lt;method&gt;GET&lt;/method&gt;
                                                &lt;status&gt;200&lt;/status&gt;
                                            &lt;/http&gt;
                                        &lt;/wait&gt;
                                    &lt;/run&gt;
                                &lt;/image&gt;
                            &lt;/images&gt;
                        &lt;/configuration&gt;
                        &lt;executions&gt;
                            &lt;execution&gt;
                                &lt;id&gt;docker-start&lt;/id&gt;
                                &lt;phase&gt;pre-integration-test&lt;/phase&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;start&lt;/goal&gt;
                                &lt;/goals&gt;
                            &lt;/execution&gt;
                            &lt;execution&gt;
                                &lt;id&gt;docker-stop&lt;/id&gt;
                                &lt;phase&gt;post-integration-test&lt;/phase&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;stop&lt;/goal&gt;
                                &lt;/goals&gt;
                            &lt;/execution&gt;
                        &lt;/executions&gt;
                    &lt;/plugin&gt;
                    &lt;plugin&gt;
                        &lt;artifactId&gt;maven-failsafe-plugin&lt;/artifactId&gt;
                        &lt;version&gt;2.19.1&lt;/version&gt;
                        &lt;executions&gt;
                            &lt;execution&gt;
                                &lt;goals&gt;
                                    &lt;goal&gt;integration-test&lt;/goal&gt;
                                    &lt;goal&gt;verify&lt;/goal&gt;
                                &lt;/goals&gt;
                            &lt;/execution&gt;
                        &lt;/executions&gt;
                    &lt;/plugin&gt;
                &lt;/plugins&gt;
            &lt;/build&gt;
        &lt;/profile&gt;</code></pre>
</div>
</div>
</li>
</ul>
</div>
<div class="paragraph">
<p>However, running Elasticsearch in docker may not be as easy as it may seem at first sight.
On many Linux boxes, the Elasticsearch container will stop immediately with this kind of error:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>[2016-11-26T14:58:32,140][INFO ][o.e.b.BootstrapCheck     ] [3mI2H8T] bound or publishing to a non-loopback or non-link-local address, enforcing bootstrap checks
ERROR: bootstrap checks failed
max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]</pre>
</div>
</div>
<div class="paragraph">
<p>When Elasticsearch 5.0 is running inside a Docker container, it doesn&#8217;t to listen on localhost interface, but on a container interface.
This network setting makes Elasticsearch think it is running in production mode.
As a consequence, Elasticsearch does some additional <a href="https://www.elastic.co/blog/bootstrap_checks_annoying_instead_of_devastating">bootstrap checks</a> to avoid common production issues.
Like on your production server, you&#8217;ll have to do some the system level tuning to allow it to start:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>sudo sysctl -w vm.max_map_count=262144</pre>
</div>
</div>
<div class="paragraph">
<p>If you don&#8217;t have sufficient privileges to change such setting, then you&#8217;re in trouble.
I personally miss a setting to be able to disable bootstrap checking.</p>
</div>
<div class="paragraph">
<p>As my colleague <a href="https://twitter.com/mickaeljeanroy/status/804263195359715328">Mickael Jeanroy pointed out</a>, we could also start this Docker container from JUnit.
There are indeed several libraries to mix Docker and JUnit: <a href="https://github.com/geowarin/docker-junit-rule">Docker JUnit Rule</a>,another <a href="https://github.com/tdomzal/junit-docker-rule">JUnit Docker Rule</a>,
<a href="https://github.com/testcontainers/testcontainers-java">Test containers</a>.
Most of these libraries are based on Spotify&#8217;s <a href="https://github.com/spotify/docker-client">Docker client for Java</a>.</p>
</div>
<div class="paragraph">
<p>With that kind of library, starting Elasticsearch becomes easy again:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">@ClassRule
public static DockerRule elasticsearchRule = DockerRule.builder()
        .imageName("elasticsearch:5.0.1")
        .mountFrom("/usr/share/elasticsearch/data").to(dataDir())
        .env("ES_JAVA_OPTS","-Xmx1g -Xms1g")
        .expose("9200", "9200")
        .waitForMessage("started")
        .waitForHttpPing(9200)
        .build();</code></pre>
</div>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/11/30/Testing-a-Java-and-Elasticsearch-50-application.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/11/30/Testing-a-Java-and-Elasticsearch-50-application.html</guid><category><![CDATA[elasticsearch]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Wed, 30 Nov 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Scaling Kafka]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>In my previous article about Kafka, I introduced some basic concepts,
and showed how to use this message broker using the Java client API.</p>
</div>
<div class="paragraph">
<p>In this article I will tackle an operational need: adding and removing nodes in a Kafka 0.10.0 cluster.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_creating_a_topic">Creating a topic</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We will start with a cluster made of 3 nodes identified 0, 1 and 2.
We first create a topic using the <code>kafka-topics.sh</code> tool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/kafka-topics.sh --zookeeper zkhost:2181 --create --topic test_topic --partitions 5 --replication-factor 2
Created topic "test_topic".

$ bin/kafka-topics.sh --zookeeper zkhost:2181 --describe --topic test_topic
Topic:test_topic        PartitionCount:5        ReplicationFactor:2     Configs:
        Topic: test_topic       Partition: 0    Leader: 2       Replicas: 2,0   Isr: 2,0
        Topic: test_topic       Partition: 1    Leader: 0       Replicas: 0,1   Isr: 0,1
        Topic: test_topic       Partition: 2    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: test_topic       Partition: 3    Leader: 2       Replicas: 2,1   Isr: 2,1
        Topic: test_topic       Partition: 4    Leader: 0       Replicas: 0,2   Isr: 0,2</code></pre>
</div>
</div>
<div class="paragraph">
<p>Our first topic has 5 partitions and a replication factor of 2
which means 10 partitions will be distributed on our 3 nodes cluster.
As you can see leader partitions and replicas are distributed homogeneously on the nodes.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-17-Scaling-Kafka/kafka-create-topic.svg" alt="Creating a topic">
</div>
</div>
<div class="paragraph">
<p>All the topic configuration is stored in Zookeeper, which makes it available to all Kafka.
This why in all the commands we will use to manage topics and their partitions there is a <code>--zookeeper</code> argument.
You can use the <code>zookeeper-shell.sh</code> tool to dig in Zookeeper tree:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/zookeeper-shell.sh zkhost:2181
Connecting to zkhost:2181
Welcome to ZooKeeper!

ls /brokers/topics
[__consumer_offsets,test_topic]

get /brokers/topics/test_topic
{"version":1,"partitions":{"4":[0,2],"1":[0,1],"0":[2,0],"2":[1,2],"3":[2,1]}}

ls /brokers/topics/test_topic/partitions
[0, 1, 2, 3, 4]

get /brokers/topics/test_topic/partitions/0/state
{"controller_epoch":1,"leader":2,"version":1,"leader_epoch":0,"isr":[2,0]}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_adding_a_broker_node">Adding a broker node</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now we will add a fourth node with Id 3 to our cluster.
Once the node is started and has successfully joined the cluster,
it doesn&#8217;t automatically receive partitions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/kafka-topics.sh --zookeeper zkhost:2181 --describe --topic test_topic
Topic:test_topic        PartitionCount:5        ReplicationFactor:2     Configs:
        Topic: test_topic       Partition: 0    Leader: 2       Replicas: 2,0   Isr: 0,2
        Topic: test_topic       Partition: 1    Leader: 0       Replicas: 0,1   Isr: 1,0
        Topic: test_topic       Partition: 2    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: test_topic       Partition: 3    Leader: 2       Replicas: 2,1   Isr: 1,2
        Topic: test_topic       Partition: 4    Leader: 0       Replicas: 0,2   Isr: 0,2</code></pre>
</div>
</div>
<div class="paragraph">
<p>So we will have to redistribute partitions on the 4 nodes.
In fact, our cluster has 2 topics:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/kafka-topics.sh --zookeeper zkhost:2181 --list
__consumer_offsets
test_topic</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p><code>test_topic</code> is the topic we created above</p>
</li>
<li>
<p><code>__consumer_offsets</code> is an internal topic used to track consumer offsets.
It has 5 partitions and a replication factor of 3:</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/kafka-topics.sh --zookeeper zkhost:2181 --describe --topic __consumer_offsets
Topic:__consumer_offsets        PartitionCount:5        ReplicationFactor:3     Configs:segment.bytes=104857600,cleanup.policy=compact,compression.type=producer
        Topic: __consumer_offsets       Partition: 0    Leader: 0       Replicas: 0,3,1 Isr: 0,3,1
        Topic: __consumer_offsets       Partition: 1    Leader: 1       Replicas: 1,0,2 Isr: 1,0,2
        Topic: __consumer_offsets       Partition: 2    Leader: 2       Replicas: 2,1,3 Isr: 2,1,3
        Topic: __consumer_offsets       Partition: 3    Leader: 3       Replicas: 3,2,0 Isr: 3,2,0
        Topic: __consumer_offsets       Partition: 4    Leader: 0       Replicas: 0,1,2 Isr: 0,1,2</code></pre>
</div>
</div>
<div class="paragraph">
<p>We need to write a JSON file to list the topics we want to reorganize:
<code>test_topic</code> and <code>__consumer_offsets</code> in our case:</p>
</div>
<div class="listingblock">
<div class="title">topics.json</div>
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "version": 1,
  "topics": [
     {"topic": "test_topic"},
     {"topic": "__consumer_offsets"}
  ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now we can use the <code>kafka-reassign-partitions.sh</code> tool to generate partition assignments.
It takes the topic list and the broker list as input, and produces the assignment plan in JSON format:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/kafka-reassign-partitions.sh --zookeeper zkhost:2181 --generate --topics-to-move-json-file topics.json --broker-list 0,1,2,3
Current partition replica assignment

{"version":1,"partitions":[{"topic":"test_topic","partition":0,"replicas":[2,0]},{"topic":"test_topic","partition":4,"replicas":[0,2]},{"topic":"__consumer_offsets","partition":0,"replicas":[0,3,1]},{"topic":"__consumer_offsets","partition":3,"replicas":[3,2,0]},{"topic":"test_topic","partition":3,"replicas":[2,1]},{"topic":"test_topic","partition":2,"replicas":[1,2]},{"topic":"__consumer_offsets","partition":4,"replicas":[0,1,2]},{"topic":"test_topic","partition":1,"replicas":[0,1]},{"topic":"__consumer_offsets","partition":2,"replicas":[2,1,3]},{"topic":"__consumer_offsets","partition":1,"replicas":[1,0,2]}]}
Proposed partition reassignment configuration

{"version":1,"partitions":[{"topic":"test_topic","partition":0,"replicas":[3,0]},{"topic":"test_topic","partition":4,"replicas":[3,1]},{"topic":"__consumer_offsets","partition":0,"replicas":[0,1,2]},{"topic":"test_topic","partition":2,"replicas":[1,2]},{"topic":"test_topic","partition":3,"replicas":[2,3]},{"topic":"__consumer_offsets","partition":3,"replicas":[3,0,1]},{"topic":"__consumer_offsets","partition":4,"replicas":[0,2,3]},{"topic":"test_topic","partition":1,"replicas":[0,1]},{"topic":"__consumer_offsets","partition":1,"replicas":[1,2,3]},{"topic":"__consumer_offsets","partition":2,"replicas":[2,3,0]}]}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Let&#8217;s use the above proposed reassignment plan, format it a bit to make it more readable,
and save it in a <code>reassignment.json</code> file:</p>
</div>
<div class="listingblock">
<div class="title">reassignment.json</div>
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "version":1,
  "partitions":[
    {"topic":"test_topic",        "partition":0,"replicas":[3,0]},
    {"topic":"test_topic",        "partition":1,"replicas":[0,1]},
    {"topic":"test_topic",        "partition":2,"replicas":[1,2]},
    {"topic":"test_topic",        "partition":3,"replicas":[2,3]},
    {"topic":"test_topic",        "partition":4,"replicas":[3,1]},
    {"topic":"__consumer_offsets","partition":0,"replicas":[0,1,2]},
    {"topic":"__consumer_offsets","partition":1,"replicas":[1,2,3]},
    {"topic":"__consumer_offsets","partition":2,"replicas":[2,3,0]},
    {"topic":"__consumer_offsets","partition":3,"replicas":[3,0,1]},
    {"topic":"__consumer_offsets","partition":4,"replicas":[0,2,3]}
  ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The aim of this file is to tell on which node each partition (leader or replica) must be located.
You can check in this assignment plan that:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>All 4 nodes 0, 1, 2 and 3 are used,</p>
</li>
<li>
<p>Each node has roughly the number of partitions: 6 or 7 (= (2&times;5 + 3&times;5) &div; 4)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To run this plan, we will use the <code>kafka-reassign-partitions.sh</code> tool with the <code>--execute</code> command.
It takes the generated <code>reassignment.json</code> file as input.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ bin/kafka-reassign-partitions.sh --zookeeper zkhost:2181 --execute --reassignment-json-file reassignment.json
Current partition replica assignment

{"version":1,"partitions":[{"topic":"test_topic","partition":0,"replicas":[2,0]},{"topic":"test_topic","partition":4,"replicas":[0,2]},{"topic":"__consumer_offsets","partition":0,"replicas":[0,3,1]},{"topic":"__consumer_offsets","partition":3,"replicas":[3,2,0]},{"topic":"test_topic","partition":3,"replicas":[2,1]},{"topic":"test_topic","partition":2,"replicas":[1,2]},{"topic":"__consumer_offsets","partition":4,"replicas":[0,1,2]},{"topic":"test_topic","partition":1,"replicas":[0,1]},{"topic":"__consumer_offsets","partition":2,"replicas":[2,1,3]},{"topic":"__consumer_offsets","partition":1,"replicas":[1,0,2]}]}

Save this to use as the --reassignment-json-file option during rollback
Successfully started reassignment of partitions {"version":1,"partitions":[{"topic":"__consumer_offsets","partition":4,"replicas":[0,2,3]},{"topic":"__consumer_offsets","partition":3,"replicas":[3,0,1]},{"topic":"__consumer_offsets","partition":0,"replicas":[0,1,2]},{"topic":"test_topic","partition":4,"replicas":[3,1]},{"topic":"test_topic","partition":3,"replicas":[2,3]},{"topic":"test_topic","partition":2,"replicas":[1,2]},{"topic":"test_topic","partition":0,"replicas":[3,0]},{"topic":"__consumer_offsets","partition":2,"replicas":[2,3,0]},{"topic":"test_topic","partition":1,"replicas":[0,1]},{"topic":"__consumer_offsets","partition":1,"replicas":[1,2,3]}]}</pre>
</div>
</div>
<div class="paragraph">
<p>You should be aware that you can not execute an assignment plan containing a dead or stopped node.
The assignment can only be executed if mentioned brokers are alive.</p>
</div>
<div class="paragraph">
<p>Once the reassignment is finished, your partitions have been redistributed over the cluster:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-17-Scaling-Kafka/kafka-add-node.svg" alt="Adding a node">
</div>
</div>
<div class="paragraph">
<p>It may take a lot of time to move partitions from one node to another when the partitions are fat.
To check the partition reassignment, you can either use:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>kafka-reassign-partitions.sh</code> tool with the <code>--verify</code> command.</p>
</li>
<li>
<p>The <code>kafka-topic.sh</code> tool with the <code>--describe</code> command.</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre>$ bin/kafka-reassign-partitions.sh --zookeeper zkhost:2181 --verify --reassignment-json-file reassignment.json
Status of partition reassignment:
Reassignment of partition [__consumer_offsets,4] completed successfully
Reassignment of partition [__consumer_offsets,3] completed successfully
Reassignment of partition [__consumer_offsets,0] completed successfully
Reassignment of partition [test_topic,4] completed successfully
Reassignment of partition [test_topic,3] completed successfully
Reassignment of partition [test_topic,2] is still in progress
Reassignment of partition [test_topic,0] completed successfully
Reassignment of partition [__consumer_offsets,2] completed successfully
Reassignment of partition [test_topic,1] is still in progress
Reassignment of partition [__consumer_offsets,1] completed successfully

$ bin/kafka-topics.sh --zookeeper zkhost:2181 --describe --topic test_topic
Topic:test_topic        PartitionCount:5        ReplicationFactor:2     Configs:
        Topic: test_topic       Partition: 0    Leader: 3       Replicas: 3,0   Isr: 0,3
        Topic: test_topic       Partition: 1    Leader: 0       Replicas: 0,1   Isr: 1,0
        Topic: test_topic       Partition: 2    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: test_topic       Partition: 3    Leader: 2       Replicas: 2,3   Isr: 2,3
        Topic: test_topic       Partition: 4    Leader: 3       Replicas: 3,1   Isr: 3,1</pre>
</div>
</div>
<div class="paragraph">
<p>Unfortunately, the tools available to monitor this reassignment are scarce,
and you have no clue about how much it will take to end.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_removing_a_broker_node">Removing a broker node</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The recipe to remove a node is very similar to the previous one:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><code>kafka-topic.sh --list</code> to get the topic list and write a <code>topics.json</code></p>
</li>
<li>
<p><code>kafka-reassign-partitions.sh --generate</code> to generate an assignment plan <code>assignment.json</code> excluding the node to remove</p>
</li>
<li>
<p><code>kafka-reassign-partitions.sh --execute</code> to run the assignment plan</p>
</li>
<li>
<p><code>kafka-reassign-partitions.sh --verify</code> to check whether the assignment plan is applied</p>
</li>
<li>
<p>Stop the broker and remove it</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>As an example, we will remove the broker with Id 1.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ bin/kafka-reassign-partitions.sh --zookeeper zkhost:2181 --generate --topics-to-move-json-file topics.json --broker-list 0,2,3</pre>
</div>
</div>
<div class="paragraph">
<p>The tool proposes the following reassignement:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "version":1,
  "partitions":[
    {"topic":"test_topic",        "partition":0,"replicas":[0,2]},
    {"topic":"test_topic",        "partition":1,"replicas":[2,3]},
    {"topic":"test_topic",        "partition":2,"replicas":[3,0]},
    {"topic":"test_topic",        "partition":3,"replicas":[0,3]},
    {"topic":"test_topic",        "partition":4,"replicas":[2,0]},
    {"topic":"__consumer_offsets","partition":0,"replicas":[2,3,0]},
    {"topic":"__consumer_offsets","partition":1,"replicas":[3,0,2]},
    {"topic":"__consumer_offsets","partition":2,"replicas":[0,2,3]},
    {"topic":"__consumer_offsets","partition":3,"replicas":[2,0,3]},
    {"topic":"__consumer_offsets","partition":4,"replicas":[3,2,0]}
  ]
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once executed, the topic is reorganized like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/kafka-topics.sh --zookeeper zkhost:2181 --describe --topic test_topic
Topic:test_topic        PartitionCount:5        ReplicationFactor:2     Configs:
        Topic: test_topic       Partition: 0    Leader: 0       Replicas: 0,2   Isr: 0,2
        Topic: test_topic       Partition: 1    Leader: 2       Replicas: 2,3   Isr: 2,3
        Topic: test_topic       Partition: 2    Leader: 3       Replicas: 3,0   Isr: 0,3
        Topic: test_topic       Partition: 3    Leader: 0       Replicas: 0,3   Isr: 3,0
        Topic: test_topic       Partition: 4    Leader: 2       Replicas: 2,0   Isr: 0,2</code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-17-Scaling-Kafka/kafka-remove-node.svg" alt="Removing a node">
</div>
</div>
<div class="paragraph">
<p>As you may observe in this example, the data movement between nodes for the partitions of the <code>test_topic</code> is not optimal.
As a result, a hand written assignment may sometimes be preferable over the generated one.</p>
</div>
<div class="paragraph">
<p>To replace a node by another one, you don&#8217;t need to use the above scenarios
because you can keep the same partition assignment.
All you have to do is:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the old node</p>
</li>
<li>
<p>Give the new node the same Id as the old one</p>
</li>
<li>
<p>Start the new node</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_rack_awareness">Rack awareness</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Starting with version 0.10.0, Kafka supports rack aware replica placement.
It means Kafka will try to place replicas in different racks (or availability zones).</p>
</div>
<div class="paragraph">
<p>The only change is the <code>broker.rack</code> property in the broker configuration file:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-properties" data-lang="properties">broker.id=0
broker.rack=A</code></pre>
</div>
</div>
<div class="paragraph">
<p>For instance, imagine brokers 0 and 1 are in rack A, while brokers 2 and 3,
are in rack B.
Now, let&#8217;s create a topic with a replication factor two,
each partition has a replica in each rack.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/kafka-topics.sh --zookeeper zkhost:2181 --create --topic test_topic --partitions 5 --replication-factor 2
Created topic "test_topic".

$ bin/kafka-topics.sh --zookeeper zkhost:2181 --describe --topic test_topic
Topic:test_topic        PartitionCount:5        ReplicationFactor:2     Configs:
        Topic: test_topic       Partition: 0    Leader: 1       Replicas: 1,3   Isr: 1,3
        Topic: test_topic       Partition: 1    Leader: 3       Replicas: 3,0   Isr: 3,0
        Topic: test_topic       Partition: 2    Leader: 0       Replicas: 0,2   Isr: 0,2
        Topic: test_topic       Partition: 3    Leader: 2       Replicas: 2,1   Isr: 2,1
        Topic: test_topic       Partition: 4    Leader: 1       Replicas: 1,2   Isr: 1,2</code></pre>
</div>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-17-Scaling-Kafka/kafka-rack.svg" alt="Rack awareness">
</div>
</div>
<div class="paragraph">
<p>This feature is really interesting to improve failure tolerance,
but it makes the assignment harder to build manually.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_simple_scaling">Simple scaling</h2>
<div class="sectionbody">
<div class="paragraph">
<p>As you have seen it, horizontally scaling a Kafka cluster is not that hard, but it is tedious.</p>
</div>
<div class="paragraph">
<p>Kafka Manager allows, through its web UI, to visually reassign partitions to nodes.</p>
</div>
<div class="paragraph">
<p>Running on a highly elastic environment, like a Docker cluster scheduler, seems sensitive.
Some solutions exist though:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Confluent Enterprise 3.1</strong> contains a feature called <a href="http://www.confluent.io/product/auto-data-balancing/">Auto data balancing</a>
whose purpose is to ease these operations.
Unfortunately, it is not open source.</p>
</li>
<li>
<p><strong>Mesos</strong> has an <a href="https://github.com/mesos/kafka">integration</a> which seems to be able to make <a href="https://docs.mesosphere.com/1.9/usage/service-guides/kafka/">Kafka scaling smoother</a></p>
</li>
</ul>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/10/17/Scaling-Kafka.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/10/17/Scaling-Kafka.html</guid><category><![CDATA[kafka]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Mon, 17 Oct 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Kafka Java Client]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Apache Kafka is trendy software which mixes a message broker and an event log.
From the ground up, it&#8217;s a distributed solution designed for scalability and performance.
It was created by LinkedIn in 2011, it is now open-source and supported by the Confluent company.</p>
</div>
<div class="paragraph">
<p>For Java developers, until Kafka 0.8, there was only an intricate Scala API with Java bindings.
Since 0.9 there is a pure Java API which makes things simpler.
In this blog post we will discuss this API.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_creating_a_topic">Creating a Topic</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The <strong>topic</strong> is the place where messages are sent and where they are stored.
All messages send to Kafka are written to disk.</p>
</div>
<div class="paragraph">
<p>A topic is split into multiple <strong>partitions</strong>.
Each partition is usually placed on a different Kafka node.
Each partition can be <strong>replicated</strong> many times in order to tolerate node failures.
Among replicas, a leader is elected, it has the privilege of receiving messages first and sending messages to consumers.</p>
</div>
<div class="paragraph">
<p>A topic is automatically created when the first message arrives.
Alternatively, it may be manually created with the <code>kafka-topic.sh</code> command line tool:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-bash" data-lang="bash">$ bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic newtopic --partitions 5 --replication-factor 2
Created topic "newtopic".
$ bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic newtopic
Topic:newtopic  PartitionCount:5        ReplicationFactor:2     Configs:
        Topic: newtopic Partition: 0    Leader: 2       Replicas: 2,3   Isr: 2,3
        Topic: newtopic Partition: 1    Leader: 3       Replicas: 3,1   Isr: 3,1
        Topic: newtopic Partition: 2    Leader: 1       Replicas: 1,2   Isr: 1,2
        Topic: newtopic Partition: 3    Leader: 2       Replicas: 2,1   Isr: 2,1
        Topic: newtopic Partition: 4    Leader: 3       Replicas: 3,2   Isr: 3,2</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, the topic has five partitions.
Partition 0 has two replicas: one on node 2, the other node 3.
The replica on node 2 is the leader.
You will notice the leader partitions are evenly distributed among this 3 node cluster.</p>
</div>
<div class="paragraph">
<p>Each partition is an independent log of events.
In this log, messages are <strong>sorted</strong> by their arrival order.
Each message is identified by its position in the log, this position is called <strong>offset</strong>.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-10-Kafka-Java-Client/kafka_topic.svg" alt="Topic" width="Partitions and Offsets">
</div>
</div>
<div class="paragraph">
<p>At the time of writing (Kafka 0.10.0), it is not possible to create or delete a Topic with the Kafka Client library.
By the way, this should change in the upcoming release (0.10.1).
Right now, you&#8217;ll have to stick with the forementioned command line tool, or use the Scala library which contains an <code>AdminUtils</code> class.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_message_record">Message / Record</h2>
<div class="sectionbody">
<div class="paragraph">
<p>A message is also called a <strong>record</strong> in the Kafka vocabulary.
It consists of:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>An optional <strong>key</strong>: it doesn&#8217;t guarantee message uniqueness, multiple messages may have the same key.</p>
</li>
<li>
<p>A <strong>value</strong>: the body, the main part of the message</p>
</li>
<li>
<p>A <strong>timestamp</strong> (since Kafka 0.10): when the message was created by the producer or recorded in the broker</p>
</li>
<li>
<p>An <strong>offset</strong>: a big number describing the position of the message in the log</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>When the key is provided, it is hashed and this hash is used to determine in which partition it should go.
Two messages with the same key will end in the same partition.
Messages having the same key can be merged together by an optional background process called compaction.</p>
</div>
<div class="paragraph">
<p>The key and the value can be of any type: <code>String</code>, <code>long</code>, <code>byte[]</code>&#8230;&#8203;
This is made possible by <strong>serializers</strong> and <strong>deserializers</strong> which are strategies to read and write anything from byte stream.
Some (de)serializers are provided for basic types,
and there are some third party (de)serializers to handle complex types.
For example, it&#8217;s possible to exchange plain objects written in <a href="https://github.com/confluentinc/schema-registry/tree/master/json-serializer">JSON</a> or Avro format.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_sending_messages">Sending messages</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Messages are sent to the Kafka broker using a <strong>producer</strong>.
The producer knows the distribution of topic partitions on nodes,
it will hash the record key and send the record directly to the appropriate partition/node.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-10-Kafka-Java-Client/kafka_producer.svg" alt="Producer">
</div>
</div>
<div class="paragraph">
<p>When no key is provided, the producer uses a random partition.
In short, the producer is able to load balance writes to all partitions, and associated nodes.</p>
</div>
<div class="paragraph">
<p>This producer is initialized and configured with some properties.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Map&lt;String, Object&gt; producerConfig = new HashMap&lt;&gt;();
producerConfig.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka1:9092,kafka2:9092"); <i class="conum" data-value="1"></i><b>(1)</b>
producerConfig.put(ProducerConfig.ACKS_CONFIG, "1"); <i class="conum" data-value="5"></i><b>(5)</b>
producerConfig.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); <i class="conum" data-value="4"></i><b>(4)</b>
producerConfig.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
try (Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(producerConfig)) {
    ProducerRecord&lt;String, String&gt; record = new ProducerRecord&lt;&gt;("the_topic", "the_key","The Message"); <i class="conum" data-value="2"></i><b>(2)</b>
    Future&lt;RecordMetadata&gt; futureMetadata = producer.send(record);<i class="conum" data-value="3"></i><b>(3)</b>
    RecordMetadata metadata = producer.get(); <i class="conum" data-value="5"></i><b>(5)</b>
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Connect to these Kafka connect nodes.
The first Kafka node to answer will give the full list of nodes.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Create the record/message</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Send the message.
By default, this operation is asynchronous and non blocking, it immediately returns a <code>Future</code></td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>The message and the key are written on the wire using the string serializer.</td>
</tr>
<tr>
<td><i class="conum" data-value="5"></i><b>5</b></td>
<td>Wait for message acknowledgement.
The <strong>Acks</strong> config indicates how many replicas should write the message to disk before returning an acknowledgement to the producer.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In a real world application, the producer should be instanciated only once and reused for the application lifespan.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_receiving_messages">Receiving messages</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Messages are received from the Kafka broker by a <strong>consumer</strong>.
A consumer is a process in charge for reading messages from a topic and dealing with them.
As an acknowledgement, the consumer writes the message offset back to the broker, it&#8217;s called <strong>offset commit</strong>.</p>
</div>
<div class="paragraph">
<p>A <strong>consumer group</strong> is a set of consumers distributed on multiple machines.
For a given topic and group, each partition gets read by a single consumer.
This prevents messages from being consumed twice in the consumer group.
On the contrary, a consumer can be in charge of several partitions.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://gquintana.github.io/images/2016-10-10-Kafka-Java-Client/kafka_consumer.svg" alt="Consumer and Consumer Group">
</div>
</div>
<div class="paragraph">
<p>The Kafka cluster tells each consumer which partition it should read from.
Each consumer takes care of its portion of topic.
As a result, consumers can work independently and in parallel,
and messages stored in a topic can be load balanced to consumers on many machines.
In case of consumer failure, Kafka reassigns the partitions to other consumers of the same group.</p>
</div>
<div class="paragraph">
<p>Like the producer, the consumer is initialized and configured with some properties.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">Map&lt;String, Object&gt; consumerConfig = new HashMap&lt;&gt;();
consumerConfig.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "kafka1:9092,kafka2:9092"); <i class="conum" data-value="1"></i><b>(1)</b>
consumerConfig.put(ConsumerConfig.GROUP_ID_CONFIG, "test_group");
consumerConfig.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
consumerConfig.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
try (Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(consumerConfig)) {
    consumer.subscribe(Arrays.asList("the_topic")); <i class="conum" data-value="2"></i><b>(2)</b>
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000L); <i class="conum" data-value="3"></i><b>(3)</b>
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        LOGGER.info("Found message {} {}", record.key(), record.value());
    }
}</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Connect to these Kafka connect nodes.
Like the producer, it doesn&#8217;t have to be the whole Kafka cluster.</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Register this application (consumer group) as a consumer for this list of topics.
In return, Kafka will assign some partitions to this consumer.</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Try to pull messages from the broker.</td>
</tr>
<tr>
<td><i class="conum" data-value="4"></i><b>4</b></td>
<td>Pulled messages are automatically acknowledged.</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>In the above example, connecting to the broker, subscribing to one or more topics,
and being assigned partitions takes time and is usually done once during application start-up.
On the contrary, the <code>poll</code> method should be run in loop.
It returns a batch of records whose size is controlled by the <code>max.poll.records</code> and <code>max.partition.fetch.bytes</code> settings.</p>
</div>
<div class="paragraph">
<p>Unlike the producer, the consumer is not thread-safe.
In order to consume records in parallel, each thread should have it&#8217;s own consumer.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_acknowledging_received_messages">Acknowledging received messages</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The message acknowledgement is called <strong>offset commit</strong>,
because Kafka keeps track of the offset of the last consumed message for each topic + partition + consumer group.</p>
</div>
<div class="paragraph">
<p>In the previous example, the offsets were automatically and periodically committed to the broker.
This auto commit is configurable through properties:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">consumerConfig.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, true);
consumerConfig.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, 1000L);
try (Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(consumerConfig)) {
    consumer.subscribe(Arrays.asList("the_topic"));
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000L);
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        LOGGER.info("Found message {} {}", record.key(), record.value());
    }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>This offset commit can also be manual in order to ensure messages are acknowledged once they have been processed.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">consumerConfig.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);
try (Consumer&lt;String, String&gt; consumer = new KafkaConsumer&lt;&gt;(consumerConfig)) {
    consumer.subscribe(Arrays.asList("the_topic"));
    ConsumerRecords&lt;String, String&gt; records = consumer.poll(1000L);
    for (ConsumerRecord&lt;String, String&gt; record : records) {
        LOGGER.info("Found message {} {}", record.key(), record.value());
    }
    consumer.commitSync();
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>This offset can even be moved forward (to skip records) and backward (to replay records):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-java" data-lang="java">    consumer.seekToBeginning(consumer.assignment());</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_a_framework">Using a framework</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You may have noticed that the consumer API is a pull API.
In a real application you&#8217;ll have to create a consuming loop in separate thread,
and build a push API.</p>
</div>
<div class="paragraph">
<p>The <a href="http://docs.spring.io/spring-kafka/docs/current/reference/html/">Spring Kafka</a> does all the heavy lifting for you
and smoothly integrates Kafka with Spring and Spring Integration:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The <code>KafkaTemplate</code> can send messages</p>
</li>
<li>
<p>The <code>KafkaListener</code> can receive message in a push manner</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This library makes Kafka usage very similar to ActiveMQ or RabbitMQ.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/10/10/Kafka-Java-Client.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/10/10/Kafka-Java-Client.html</guid><category><![CDATA[kafka]]></category><category><![CDATA[java]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Mon, 10 Oct 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Testing Logstash configuration]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>You wrote a piece of Logstash configuration which can parse some logs.
You tested several corner cases to ensure the output in Elasticsearch was alright.
How do you protect this clever configuration file against regressions?</p>
</div>
<div class="paragraph">
<p>Unit testing to the rescue of course!</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_simple_example">Simple example</h2>
<div class="sectionbody">
<div class="paragraph">
<p>For the sake of simplicity, we will take an obvious example: access logs.
The input looks like</p>
</div>
<div class="listingblock">
<div class="content">
<pre>172.17.0.1 - - [05/Sep/2016:20:06:17 +0000] "GET /images/logos/hubpress.png HTTP/1.1" 200 5432 "http://localhost/" "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/51.0.2704.79 Chrome/51.0.2704.79 Safari/537.36" "-"</pre>
</div>
</div>
<div class="paragraph">
<p>The output, once in Elasticsearch, should look like</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-json" data-lang="json">{ "@version":"1",
  "@timestamp":"2016-09-05T20:06:17.000Z",
  "type":"nginx",
  "host":"nginx-server", "path":"/var/log/nginx/access.log",
  "clientip":"172.17.0.1", "ident":"-", "auth":"-",
  "verb":"GET","request":"/images/logos/hubpress.png","httpversion":"1.1",
  "response":200, "bytes":5432, "referrer":"\"http://localhost/\"",
  "agent": "\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/51.0.2704.79 Chrome/51.0.2704.79 Safari/537.36\""
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The configuration could look like</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby">input {
    file {
        path =&gt; "/var/log/nginx/access*.log"
        type =&gt; "nginx"
    }
}
filter {
    if [type] == "nginx" {
        grok {
            match =&gt; [ "message" , "%{COMBINEDAPACHELOG}"]
        }
        date {
            match =&gt; [ "timestamp" , "dd/MMM/YYYY:HH:mm:ss Z" ]
        }
        mutate {
            convert =&gt; ["response", "integer"]
            convert =&gt; ["bytes", "integer"]
        }
    }
}
output {
    elasticsearch {
      hosts =&gt; [ "es-server"]
      index =&gt; "logstash-%{+YYYY.MM.dd}"
      document_type =&gt; "%{type}"
    }
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_split_the_file">Split the file</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In the above config file, the interesting part, the one containing logic is the filter part.
In order to test it, the first thing to do is split this big file into small pieces:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>01_logstash_input_nginx.conf</code> contains the nginx file input</p>
</li>
<li>
<p><code>02_logstash_filter_nginx.conf</code> contains the nginx filter section</p>
</li>
<li>
<p><code>03_logstash_output.conf</code> contains the elasticsearch output</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In production, you can load multiple config files as if they were a single one:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>logstash agent -f /etc/logstash.d/*.conf"</pre>
</div>
</div>
<div class="paragraph">
<p>At test time, by picking a single configuration file <code>02_logstash_filter_nginx.conf</code>, the Nginx log parsing can be tested in isolation.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_write_the_unit_test">Write the unit test</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now let&#8217;s test the <code>02_logstash_filter_nginx.conf</code> file alone and write a simple Ruby test case.
As you may know, Logstash is written in JRuby.</p>
</div>
<div class="listingblock">
<div class="title">02_logstash_filter_nginx_spec.rb</div>
<div class="content">
<pre class="highlight"><code class="language-ruby" data-lang="ruby"># encoding: utf-8
require "logstash/devutils/rspec/spec_helper"

# Load the configuration file
@@configuration = String.new
@@configuration &lt;&lt; File.read("conf/02_logstash_nginx_filter.conf")

describe "Nginx filter" do

  config(@@configuration) <i class="conum" data-value="1"></i><b>(1)</b>

  # Inject input event/message into the pipeline
  message = "172.17.0.1 - - [05/Sep/2016:20:06:17 +0000] \"GET /images/logos/hubpress.png HTTP/1.1\" 200 5432 \"http://localhost/\" \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/51.0.2704.79 Chrome/51.0.2704.79 Safari/537.36\" \"-\""
  sample("message" =&gt; message, "type" =&gt; "nginx") do <i class="conum" data-value="2"></i><b>(2)</b>
    # Check the ouput event/message properties
    insist { subject.get("type") } == "nginx" <i class="conum" data-value="3"></i><b>(3)</b>
    insist { subject.get("@timestamp").to_iso8601 } == "2016-09-05T20:06:17.000Z"
    insist { subject.get("verb") } == "GET"
    insist { subject.get("request") } == "/images/logos/hubpress.png"
    insist { subject.get("response") } == 200
    insist { subject.get("bytes") } == 5432
    reject { subject.get("tags").include?("_grokparsefailure") }
    reject { subject.get("tags").include?("_dateparsefailure") }
  end
end</code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Load configuration file</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>Inject input event/message into the pipeline</td>
</tr>
<tr>
<td><i class="conum" data-value="3"></i><b>3</b></td>
<td>Check the ouput event/message properties</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>This test uses the JRuby testing framework called RSpec (<code>describe</code> method).
The <code>config</code> and <code>sample</code> functions are located in the <a href="https://github.com/elastic/logstash-devutils">Logstash DevUtils</a> library.
The <code>insist</code> and <code>reject</code> functions are part of the <a href="https://github.com/jordansissel/ruby-insist">Ruby Insist</a> assertion library.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_run_the_unit_tests">Run the unit tests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First we will need to download and install additional development libraries like those mentioned above.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ logstash-2.4.0/bin/logstash-plugin install --development
Installing logstash-devutils, logstash-input-generator, logstash-codec-json, logstash-output-null, logstash-filter-mutate, flores, rspec, stud, pry, rspec-wait, childprocess, ftw, logstash-output-elasticsearch, rspec-sequencing, gmetric, gelf, timecop, jdbc-derby, docker-api, logstash-codec-plain, simplecov, coveralls, longshoreman, rumbster, logstash-filter-kv, logstash-filter-ruby, sinatra, webrick, poseidon, logstash-output-lumberjack, webmock, logstash-codec-line, logstash-filter-grok
Installation successful</pre>
</div>
</div>
<div class="paragraph">
<p>Now we can run the test, Logstash comes with a <code>rspec</code> command to run these spec files.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ logstash-2.4.0/bin/rspec 02_logstash_nginx_filter_spec.rb
Using Accessor#strict_set for specs
Run options: exclude {:redis=&gt;true, :socket=&gt;true, :performance=&gt;true, :couchdb=&gt;true, :elasticsearch=&gt;true, :elasticsearch_secure=&gt;true, :export_cypher=&gt;true, :integration=&gt;true, :windows=&gt;true}
.

Finished in 0.115 seconds (files took 0.784 seconds to load)
1 example, 0 failures

Randomized with seed 4384</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>rspec</code> command can also run multiple tests at once.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ logstash-2.4.0/bin/rspec spec -P '**/*_spec.rb'</pre>
</div>
</div>
<div class="paragraph">
<p>To prevent test dependencies, they are randomly ordered: This called randomized testing.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_give_me_the_code">Give me the code!</h2>
<div class="sectionbody">
<div class="paragraph">
<p>All the code shown in this article is available in <a href="https://github.com/gquintana/gquintana.github.io/tree/master/sources/2016-09-07-Testing-Logstash-configuration">Github</a>.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/09/07/Testing-Logstash-configuration.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/09/07/Testing-Logstash-configuration.html</guid><category><![CDATA[logstash]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Wed, 07 Sep 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Securing remote JMX]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Like it or not, JMX is one the main tools for JVM monitoring.
If you are using Tomcat, Kafka or Cassandra you&#8217;ll have to setup JMX tools to monitor them.</p>
</div>
<div class="paragraph">
<p>But JMX has several drawbacks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>It&#8217;s based on the obsolete RMI protocol</p>
</li>
<li>
<p>It can trigger harmful functions like garbage collection</p>
</li>
<li>
<p>It can be used for nasty exploits like <a href="https://issues.apache.org/jira/browse/COLLECTIONS-580">invoking arbitrary code</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>So securing JMX is not an option.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_going_remote">Going remote</h2>
<div class="sectionbody">
<div class="paragraph">
<p>By default, JMX is only locally accessible and secure: It can be accessed through Unix sockets.
This means you need to have access to the machine and run JMX tools with the same user as your application.
It&#8217;s usually enough for development but not for production.</p>
</div>
<div class="paragraph">
<p>To enable remote JMX, the documentations tells you turn on some JVM flags:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.jmxremote=true \
     -Dcom.sun.management.jmxremote.port=31419 \
     -Dcom.sun.management.jmxremote.ssl=false \
     -Dcom.sun.management.jmxremote.authenticate=false \
     Tomcat</pre>
</div>
</div>
<div class="paragraph">
<p>The JVM will starting listening on <strong>0.0.0.0:31419</strong> for JMX requests.
Anybody will be able to plug any JMX tool (JConsole, JVisualVM, Mission Control&#8230;&#8203;) from a remote machine.</p>
</div>
<div class="paragraph">
<p>If you have a firewall (IPTable or the like) to protect your server, and opened the 31419 TCP port,
or connecting through a tunnel, the JMX tools may fail to connect to the server.
At first, the JMX client connects to port <strong>31419</strong>, but gets redirected to a randomly chosen port (RMI WTF!).
To prevent this behaviour and force the JVM to use a single port:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.jmxremote=true \
     -Dcom.sun.management.jmxremote.port=31419 \
     -Dcom.sun.management.jmxremote.rmi.port=31419 \
     -Dcom.sun.management.jmxremote.ssl=false \
     -Dcom.sun.management.jmxremote.authenticate=false \
     Tomcat</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_file_based_authentication">File based authentication</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This configuration is nice for debugging but not secure at all.
Let&#8217;s add authentication on this JMX connection.</p>
</div>
<div class="paragraph">
<p>First create a <strong>password</strong> file, similar to <strong>/etc/password</strong>, containing login/password pairs:</p>
</div>
<div class="listingblock">
<div class="title">jmxremote.password</div>
<div class="content">
<pre class="highlight"><code>admin  adminpassword
user   userpassword</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then create an <strong>access</strong> file, similar to <strong>/etc/groups</strong>, containing login/group pairs:</p>
</div>
<div class="listingblock">
<div class="title">jmxremote.access</div>
<div class="content">
<pre class="highlight"><code>admin readwrite <i class="conum" data-value="1"></i><b>(1)</b>
user  readonly <i class="conum" data-value="2"></i><b>(2)</b></code></pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>admin has read/write access</td>
</tr>
<tr>
<td><i class="conum" data-value="2"></i><b>2</b></td>
<td>user  has read-only access</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>These two files should have limited access rights.</p>
</div>
<div class="paragraph">
<p>Finally, tell the JVM to use these files to authenticate users:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.jmxremote=true \
     -Dcom.sun.management.jmxremote.port=31419 \
     -Dcom.sun.management.jmxremote.rmi.port=31419 \
     -Dcom.sun.management.jmxremote.ssl=false \
     -Dcom.sun.management.jmxremote.password.file=/path/to/jmxremote.password \
     -Dcom.sun.management.jmxremote.access.file=/path/to/jmxremote.access \
     Tomcat</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_using_ssl">Using SSL</h2>
<div class="sectionbody">
<div class="paragraph">
<p>With this configuration, JVM tools have to provide a login and password to access to MBeans.
But, the password is sent over the wire without encryption.
Let&#8217;s connect to JMX though SSL.</p>
</div>
<div class="paragraph">
<p>You may already known that, in the Java land,
private keys and trusted certificates are stored in wallets known as <strong>keystores</strong>,
and using the <strong>JKS</strong> file format.
A tool named <strong>keytool</strong> provided with the JDK is used to import/export keys and certs in the keystore.</p>
</div>
<div class="paragraph">
<p>Once the <strong>keystore</strong> (containing private key matching the server name along with certificate chain)
and the <strong>truststore</strong> (containing trusted certificates) are built,
just reference them:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.jmxremote=true \
     -Dcom.sun.management.jmxremote.port=31419 \
     -Dcom.sun.management.jmxremote.rmi.port=31419 \
     -Dcom.sun.management.jmxremote.password.file=/path/to/jmxremote.password \
     -Dcom.sun.management.jmxremote.access.file=/path/to/jmxremote.access \
     -Dcom.sun.management.jmxremote.registry.ssl=true \
     -Djavax.net.ssl.keyStore=/path/to/keystore.jks \
     -Djavax.net.ssl.keyStorePassword=keystore_password \
     -Djavax.net.ssl.trustStore=/path/to/truststore.jks \
     -Djavax.net.ssl.trustStorePassword=truststore_password \
     Tomcat</pre>
</div>
</div>
<div class="paragraph">
<p>To connect, the JMX tools using SSL, you&#8217;ll have to provide the trusted certificates.
For instance, to start the <strong>JConsole</strong>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>jconsole \
     -J-Djavax.net.ssl.trustStore=/path/to/truststore.jks \
     -J-Djavax.net.ssl.trustStorePassword=truststore_password</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_jmx_configuration_file">JMX configuration file</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Again, this configuration has problem:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Having passwords on the command line is not a good option,
because it&#8217;s easy to use <code>ps</code> command to grab them.</p>
</li>
<li>
<p>Having some many options on the command like is not elegant</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let&#8217;s place all these options in a dedicated file:</p>
</div>
<div class="listingblock">
<div class="title">jmxremote.properties</div>
<div class="content">
<pre>com.sun.management.jmxremote=true
com.sun.management.jmxremote.port=31419
com.sun.management.jmxremote.rmi.port=31419
com.sun.management.jmxremote.password.file=/path/to/jmxremote.password
com.sun.management.jmxremote.access.file=/path/to/jmxremote.access
com.sun.management.jmxremote.registry.ssl=true
com.sun.management.jmxremote.ssl.config.file=/path/to/jmxremote.properties <i class="conum" data-value="1"></i><b>(1)</b>

javax.net.ssl.keyStore=/path/to/keystore.jks
javax.net.ssl.keyStorePassword=keystore_password
javax.net.ssl.trustStore=/path/to/truststore.jks
javax.net.ssl.trustStorePassword=truststore_password</pre>
</div>
</div>
<div class="colist arabic">
<table>
<tr>
<td><i class="conum" data-value="1"></i><b>1</b></td>
<td>Path to file containing <code>javax.net.ssl.*</code> properties</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>And now, the command line sums up to</p>
</div>
<div class="listingblock">
<div class="content">
<pre>java -Dcom.sun.management.config.file=/path/to/jmxremote.properties \
     Tomcat</pre>
</div>
</div>
<div class="paragraph">
<p>Such a JMX configuration file already exists in your JRE, it&#8217;s named <code>JRE/lib/management/management.properties</code>.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_some_pointers">Some pointers</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p><a href="https://docs.oracle.com/javase/8/docs/technotes/guides/management/">Java 8 JMX</a></p>
</li>
<li>
<p><a href="https://www.jtips.info/index.php?title=JMX/Remote">JTips</a> in French</p>
</li>
</ul>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/09/01/Securing-remote-JMX.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/09/01/Securing-remote-JMX.html</guid><category><![CDATA[java]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Thu, 01 Sep 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[Elasticsearch and YAML]]></title><description><![CDATA[<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>Most examples in Elasticsearch documentation are using JSON to represent documents, requests and responses.
Writing JSON is not that hard but you sometimes become Raiders of the Lost Curly Brace.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="https://upload.wikimedia.org/wikipedia/en/4/4c/Raiders_of_the_Lost_Ark.jpg" alt="Raiders of the Lost Ark">
</div>
</div>
<div class="paragraph">
<p>Do you know that you can replace JSON by YAML?</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_in_responses">In responses</h2>
<div class="sectionbody">
<div class="paragraph">
<p>You can add <code>format=yaml</code> in the query params:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -XGET http://localhost:9200/logstash-*/_search?format=yaml

---
took: 13
timed_out: false
_shards:
  total: 15
  successful: 15
  failed: 0
hits:
  total: 2382
  max_score: 1.0
  hits:
  - _index: "logstash-2016.06.22"
    _type: "test"
    _id: "AVV36B3vd6qInGJw3x0r"
    _score: 1.0
    _source:
      '@timestamp': "2016-06-22T11:39:07.431Z"
  - _index: "logstash-2016.06.22"
    _type: "test"
    _id: "AVV36Cfrd6qInGJw3x0u"
    _score: 1.0
    _source:
      '@timestamp': "2016-06-22T11:38:42.000Z"</pre>
</div>
</div>
<div class="paragraph">
<p>You can also use the <code>Content-Type: application/yaml</code> HTTP header:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -XGET -H "Content-Type: application/yaml" http://localhost:9200/_cluster/health

---
cluster_name: "elasticsearch"
status: "yellow"
timed_out: false
number_of_nodes: 1
number_of_data_nodes: 1
...</pre>
</div>
</div>
<div class="paragraph">
<p>While using YAML format for responses you don&#8217;t have to use the <code>pretty=true</code> query param,
as YAML is naturally indented and human readable.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_in_requests">In requests</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Just start your request body with <code>---</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -XPOST http://localhost:9200/_search -d '---
query:
  match:
    message: "Elasticsearch"
'</pre>
</div>
</div>
<div class="paragraph">
<p>This gets pretty handy when dealing with mappings and settings:</p>
</div>
<div class="listingblock">
<div class="title">movie.mapping.yaml</div>
<div class="content">
<pre class="highlight"><code class="language-yaml" data-lang="yaml">---
properties:
  title:
    type: string
  tags:
    type: string
    index: not_analyzed
  year:
    type: date
    format: year</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre>curl -XPUT http://localhost:9200/movies/_mappings/movie --data-binary @movie.mapping.yaml</pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_limits">Limits</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Some requests like <code>_bulk</code> request may not accept YAML.</p>
</div>
<div class="paragraph">
<p>Sense (Kibana Console) and other Elasticsearch UIs do not handle YAML properly.</p>
</div>
</div>
</div>]]></description><link>https://gquintana.github.io/2016/08/25/Elasticsearch-and-YAML.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/08/25/Elasticsearch-and-YAML.html</guid><category><![CDATA[elasticsearch]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Thu, 25 Aug 2016 00:00:00 GMT</pubDate></item><item><title><![CDATA[First post]]></title><description><![CDATA[<div class="paragraph">
<p>This is my first post on this blog.</p>
</div>
<div class="paragraph">
<p>I am trying out <a href="http://hubpress.io">HubPress.io</a> and I&#8217;m just amazed:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>5 minutes to setup un blog site on GitHub.</p>
</li>
<li>
<p>A neat backoffice to administrate it</p>
</li>
<li>
<p>Posts written in AsciiDoc with live preview.</p>
</li>
<li>
<p>Built with cool tech: React, Redux,</p>
</li>
<li>
<p>Mostly built in France: cock-a-doodle-doo</p>
</li>
</ul>
</div>]]></description><link>https://gquintana.github.io/2016/08/23/First-post.html</link><guid isPermaLink="true">https://gquintana.github.io/2016/08/23/First-post.html</guid><category><![CDATA[mood]]></category><dc:creator><![CDATA[Gerald Quintana]]></dc:creator><pubDate>Tue, 23 Aug 2016 00:00:00 GMT</pubDate></item></channel></rss>